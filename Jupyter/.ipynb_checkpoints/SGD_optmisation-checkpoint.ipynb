{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Optmisation of SGD\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, accuracy_score, f1_score, recall_score, precision_score, roc_auc_score\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set __location__ depeneing which operating system user is on \n",
    "\n",
    "if os.name == \"nt\": \n",
    "    __location__ = \"F:\\FinalYearProject\\Machine Learning\\Features\\CIC_dataset\\\\\"\n",
    "else:\n",
    "    __location__ = \"/media/jackp/JACK PYE/FinalYearProject/Machine Learning/Features/CIC_dataset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset into panda dataframe\n",
    "df = pd.read_csv(os.path.join(__location__,\"all_features.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   AbsListView  AbsListView.LayoutParams  AbsListView.MultiChoiceModeListener  \\\n",
      "0            1                         0                                    0   \n",
      "1            1                         0                                    0   \n",
      "2            0                         0                                    0   \n",
      "3            1                         0                                    0   \n",
      "4            0                         0                                    0   \n",
      "\n",
      "   AbsListView.OnScrollListener  AbsListView.RecyclerListener  \\\n",
      "0                             0                             0   \n",
      "1                             0                             0   \n",
      "2                             0                             0   \n",
      "3                             0                             0   \n",
      "4                             0                             0   \n",
      "\n",
      "   AbsListView.SelectionBoundsAdjuster  AbsoluteLayout  \\\n",
      "0                                    0               0   \n",
      "1                                    0               1   \n",
      "2                                    0               0   \n",
      "3                                    0               0   \n",
      "4                                    0               0   \n",
      "\n",
      "   AbsoluteLayout.LayoutParams  AbsoluteSizeSpan  AbsSavedState  ...  \\\n",
      "0                            0                 0              0  ...   \n",
      "1                            0                 0              0  ...   \n",
      "2                            0                 0              0  ...   \n",
      "3                            0                 1              0  ...   \n",
      "4                            0                 0              0  ...   \n",
      "\n",
      "   android.permission.WRITE_GSERVICES  android.permission.WRITE_MEDIA_STORAGE  \\\n",
      "0                                   0                                       0   \n",
      "1                                   0                                       0   \n",
      "2                                   0                                       0   \n",
      "3                                   0                                       1   \n",
      "4                                   0                                       0   \n",
      "\n",
      "   android.permission.WRITE_PROFILE  android.permission.WRITE_SECURE_SETTINGS  \\\n",
      "0                                 0                                         0   \n",
      "1                                 0                                         0   \n",
      "2                                 0                                         0   \n",
      "3                                 0                                         1   \n",
      "4                                 0                                         0   \n",
      "\n",
      "   android.permission.WRITE_SETTINGS  android.permission.WRITE_SMS  \\\n",
      "0                                  0                             0   \n",
      "1                                  0                             0   \n",
      "2                                  0                             0   \n",
      "3                                  1                             0   \n",
      "4                                  0                             0   \n",
      "\n",
      "   android.permission.WRITE_SOCIAL_STREAM  \\\n",
      "0                                       0   \n",
      "1                                       0   \n",
      "2                                       0   \n",
      "3                                       0   \n",
      "4                                       0   \n",
      "\n",
      "   android.permission.WRITE_SYNC_SETTINGS  \\\n",
      "0                                       0   \n",
      "1                                       0   \n",
      "2                                       0   \n",
      "3                                       1   \n",
      "4                                       0   \n",
      "\n",
      "   android.permission.WRITE_USER_DICTIONARY  malware  \n",
      "0                                         0   Benign  \n",
      "1                                         0  Malware  \n",
      "2                                         0  Malware  \n",
      "3                                         0  Malware  \n",
      "4                                         0   Benign  \n",
      "\n",
      "[5 rows x 4831 columns]\n",
      "removed 3568 Features\n"
     ]
    }
   ],
   "source": [
    "#Exploring the first five entries of the dataset to show the features that have been collected\n",
    "print(df.head())\n",
    "old_shape = df.shape\n",
    "#remove features that are not supported \n",
    "df = df.loc[:, (df.sum(axis=0) != 0)]\n",
    "new_shape = df.shape\n",
    "print(f'removed {old_shape[1] - new_shape[1]} Features')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old number of features: 1262\n",
      "             New number of features: 498\n",
      "             Number of features removed 764\n"
     ]
    }
   ],
   "source": [
    "X = df.drop('malware', axis=1)\n",
    "y = df['malware']\n",
    "## Binarize the output\n",
    "y = label_binarize(y, classes=['Benign', 'Malware'])\n",
    "y = y.ravel()\n",
    "\n",
    "\n",
    "variance = 0.1\n",
    "\n",
    "def getVarianceFeatures(X, y, var):\n",
    "    variancefs = VarianceThreshold(threshold=var).fit(X)\n",
    "    cols = variancefs.get_support(indices=True)\n",
    "    new_X = X.iloc[:,cols]\n",
    "    lostFeatures = X.shape[1] - new_X.shape[1]\n",
    "    \n",
    "    print(f'Old number of features: {X.shape[1]}\\n \\\n",
    "            New number of features: {new_X.shape[1]}\\n \\\n",
    "            Number of features removed {lostFeatures}')\n",
    "    \n",
    "    return new_X\n",
    "\n",
    "\n",
    "\n",
    "X = getVarianceFeatures(X, y, variance)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, \n",
    "                                                    random_state = 101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9393939393939394\n",
      "f1: 0.8923076923076924\n",
      "roc_auc: 0.9358100890207716\n",
      "recall: 0.928\n",
      "precision: 0.8592592592592593\n",
      "[[318  19]\n",
      " [  9 116]]\n"
     ]
    }
   ],
   "source": [
    "baseMetrics = {}\n",
    "Optimisers = {'Base Metrics': baseMetrics}\n",
    "clf = SGDClassifier()\n",
    "clf.fit(X_train,y_train)\n",
    "pred = clf.predict(X_test)\n",
    "\n",
    "baseMetrics['accuracy'] = accuracy_score(y_test, pred)\n",
    "baseMetrics['f1'] = f1_score(y_test, pred)\n",
    "baseMetrics['roc_auc'] = roc_auc_score(y_test, pred)\n",
    "baseMetrics['recall'] = recall_score(y_test, pred)\n",
    "baseMetrics['precision'] = precision_score(y_test, pred)\n",
    "\n",
    "\n",
    "for metric, score in baseMetrics.items():\n",
    "    print(f'{metric}: {score}')\n",
    "    \n",
    "print(confusion_matrix(y_test,pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RandomSearch(clf, search_space, X_train, X_test, y_train, y_test):\n",
    "    randomSearchMetrics = {}\n",
    "    clf = clf\n",
    "\n",
    "    randomModel = RandomizedSearchCV(estimator = clf, param_distributions = search_space, n_iter = 80, \n",
    "                                   cv = 10, verbose= 5, random_state= 101, n_jobs = -1)\n",
    "\n",
    "\n",
    "\n",
    "    randomModel.fit(X_train,y_train)\n",
    "    pred = randomModel.predict(X_test)\n",
    "\n",
    "    print(randomModel.best_params_)\n",
    "\n",
    "\n",
    "    randomSearchMetrics['accuracy'] = accuracy_score(y_test, pred)\n",
    "    randomSearchMetrics['f1'] = f1_score(y_test, pred)\n",
    "    randomSearchMetrics['roc_auc'] = roc_auc_score(y_test, pred)\n",
    "    randomSearchMetrics['recall'] = recall_score(y_test, pred)\n",
    "    randomSearchMetrics['precision'] = precision_score(y_test, pred)\n",
    "\n",
    "\n",
    "    return  randomSearchMetrics, randomModel.best_params_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 80 candidates, totalling 800 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done 156 tasks      | elapsed:    4.0s\n",
      "[Parallel(n_jobs=-1)]: Done 352 tasks      | elapsed:   11.0s\n",
      "[Parallel(n_jobs=-1)]: Done 644 tasks      | elapsed:   17.1s\n",
      "[Parallel(n_jobs=-1)]: Done 800 out of 800 | elapsed:   19.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'power_t': 1.0, 'penalty': 'l1', 'loss': 'log', 'learning_rate': 'constant', 'l1_ratio': 0.25, 'fit_intercept': True, 'eta0': 0.01, 'alpha': 0.001}\n",
      "accuracy: 0.9415584415584416\n",
      "f1: 0.891566265060241\n",
      "roc_auc: 0.9247121661721068\n",
      "recall: 0.888\n",
      "precision: 0.8951612903225806\n"
     ]
    }
   ],
   "source": [
    "search_space = {'loss': ['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron'],\n",
    "                'penalty': ['l2', 'l1', 'elasticnet'],\n",
    "                'alpha': [0.0001, 0.001, 0.01],\n",
    "                'learning_rate': ['invscaling', 'constant', 'optimal'],\n",
    "                'fit_intercept': [True, False],\n",
    "                'l1_ratio': [0.15, 0.25, 0.0, 1.0, 0.75, 0.5],\n",
    "                'eta0': [0.01, 0.1, 1.0],\n",
    "                'power_t': [0.5, 0.0, 1.0, 0.1, 100.0, 10.0, 50.0]\n",
    "                }\n",
    "\n",
    "randomSearchMetrics, randomModel = RandomSearch(SGDClassifier(), search_space, X_train, X_test, y_train, y_test)\n",
    "Optimisers['Random Search'] = randomSearchMetrics\n",
    "for metric, score in randomSearchMetrics.items():\n",
    "    print(f'{metric}: {score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 54 candidates, totalling 540 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  12 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done 120 tasks      | elapsed:    7.3s\n",
      "[Parallel(n_jobs=-1)]: Done 300 tasks      | elapsed:   18.0s\n",
      "[Parallel(n_jobs=-1)]: Done 533 out of 540 | elapsed:   28.6s remaining:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 540 out of 540 | elapsed:   28.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.0005, 'eta0': 0.005, 'fit_intercept': True, 'l1_ratio': 0.125, 'learning_rate': 'constant', 'loss': 'log', 'penalty': 'l1', 'power_t': 2.0}\n",
      "accuracy: 0.9415584415584416\n",
      "f1: 0.8941176470588236\n",
      "roc_auc: 0.9322611275964392\n",
      "recall: 0.912\n",
      "precision: 0.8769230769230769\n",
      "[[321  16]\n",
      " [ 11 114]]\n"
     ]
    }
   ],
   "source": [
    "gridSearchMetrics = {}\n",
    "Optimisers['Grid Search'] = gridSearchMetrics\n",
    "\n",
    "#use features of random search to refine grid search\n",
    "#divisions rounded up to nearest whole int to avoid floats\n",
    "\n",
    "\n",
    "grid_search = {\n",
    "    'loss': [randomModel['loss']],\n",
    "    'penalty': [randomModel['penalty']],\n",
    "    'alpha': [randomModel['alpha'] / 2 , randomModel['alpha'], randomModel['alpha'] * 2],\n",
    "    'learning_rate': [ randomModel['learning_rate']],\n",
    "    'fit_intercept': [randomModel['fit_intercept']],\n",
    "    'l1_ratio': [randomModel['l1_ratio'] / 2,  randomModel['l1_ratio']],\n",
    "    'eta0': [randomModel['eta0'] / 2,  randomModel['eta0'], randomModel['eta0'] * 2],\n",
    "    'power_t': [randomModel['power_t'] / 2,  randomModel['power_t'], randomModel['power_t'] * 2]\n",
    "                     \n",
    "}\n",
    "\n",
    "Gridmodel = GridSearchCV(estimator = clf, param_grid = grid_search, \n",
    "                               cv = 10, verbose= 5, n_jobs = -1)\n",
    "Gridmodel.fit(X_train,y_train)\n",
    "\n",
    "pred = Gridmodel.best_estimator_.predict(X_test)\n",
    "gridSearchMetrics['accuracy'] = accuracy_score(y_test, pred)\n",
    "gridSearchMetrics['f1'] = f1_score(y_test, pred)\n",
    "gridSearchMetrics['roc_auc'] = roc_auc_score(y_test, pred)\n",
    "gridSearchMetrics['recall'] = recall_score(y_test, pred)\n",
    "gridSearchMetrics['precision'] = precision_score(y_test, pred)\n",
    "\n",
    "print(Gridmodel.best_params_) \n",
    "\n",
    "for metric, score in gridSearchMetrics.items():\n",
    "    print(f'{metric}: {score}')\n",
    "    \n",
    "print(confusion_matrix(y_test,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [01:32<00:00,  2.16trial/s, best loss: -0.9432762201453789]\n",
      "{'alpha': 0.03373491226602915, 'eta0': 0.2861284204690144, 'fit_intercept': 1, 'l1_ratio': 0.7121130554265984, 'learning_rate': 1, 'loss': 3, 'penalty': 1, 'power_t': 83.5692620195492}\n"
     ]
    }
   ],
   "source": [
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
    "\n",
    "\n",
    "\n",
    "space = {'loss': hp.choice('loss', ['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron']),\n",
    "        'penalty': hp.choice('penalty', ['l2', 'l1', 'elasticnet']),\n",
    "        'alpha': hp.uniform('alpha', 0.0, 0.1),\n",
    "        'learning_rate': hp.choice ('learning_rate', ['invscaling', 'constant']),\n",
    "        'fit_intercept' : hp.choice ('fit_intercept',[True, False]),\n",
    "        'l1_ratio' : hp.uniform('l1_ratio', 0.1, 1.0),\n",
    "        'eta0' : hp.uniform('eta0', 0.01, 1.0),\n",
    "        'power_t' : hp.uniform('power_t', 0.0, 100.0)\n",
    "    }\n",
    "\n",
    "def objective(space):\n",
    "    model = SGDClassifier(loss = space['loss'], \n",
    "                          penalty = space['penalty'],\n",
    "                          alpha = space['alpha'],\n",
    "                          learning_rate = space['learning_rate'],\n",
    "                          fit_intercept = space['fit_intercept'],\n",
    "                          l1_ratio = space['l1_ratio'],\n",
    "                          eta0 = space['eta0'], \n",
    "                          power_t = space['power_t'],\n",
    "                         )\n",
    "    \n",
    "    accuracy = cross_val_score(model, X_train, y_train, cv = 10).mean()\n",
    "\n",
    "    # We aim to maximize accuracy, therefore we return it as a negative value\n",
    "    return {'loss': -accuracy, 'status': STATUS_OK }\n",
    "    \n",
    "trials = Trials()\n",
    "best = fmin(fn= objective,\n",
    "            space= space,\n",
    "            algo= tpe.suggest,\n",
    "            max_evals = 200,\n",
    "            trials= trials)\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9307359307359307\n",
      "f1: 0.8814814814814814\n",
      "roc_auc: 0.9374243323442136\n",
      "recall: 0.952\n",
      "precision: 0.8206896551724138\n",
      "[[311  26]\n",
      " [  6 119]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "bayesianMetrics = {}\n",
    "Optimisers['Bayesian'] = bayesianMetrics\n",
    "\n",
    "#convert the ints back into parameters\n",
    "loss_ = {0: 'hinge', 1: 'log', 2: 'modified_huber', 3: 'squared_hinge', 4: 'perceptron'}\n",
    "penalty_ = {0: 'l2', 1: 'l1', 2: 'elasticnet'}\n",
    "learning_rate_ = {0: 'invscaling', 1: 'constant'}\n",
    "fit_intercept_ = {0: True, 1: False}\n",
    "\n",
    "\n",
    "trainedforest = SGDClassifier(loss = loss_[best['loss']], \n",
    "                              penalty = penalty_[best['penalty']], \n",
    "                              alpha = best['alpha'], \n",
    "                              learning_rate = learning_rate_[best['learning_rate']], \n",
    "                              fit_intercept = fit_intercept_[best['fit_intercept']], \n",
    "                              l1_ratio = best['l1_ratio'],\n",
    "                              eta0 = best['eta0'],\n",
    "                              power_t = best['power_t']\n",
    "                            ).fit(X_train,y_train)\n",
    "pred = trainedforest.predict(X_test)\n",
    "\n",
    "bayesianMetrics['accuracy'] = accuracy_score(y_test, pred)\n",
    "bayesianMetrics['f1'] = f1_score(y_test, pred)\n",
    "bayesianMetrics['roc_auc'] = roc_auc_score(y_test, pred)\n",
    "bayesianMetrics['recall'] = recall_score(y_test, pred)\n",
    "bayesianMetrics['precision'] = precision_score(y_test, pred)\n",
    "\n",
    "for metric, score in bayesianMetrics.items():\n",
    "    print(f'{metric}: {score}')\n",
    "    \n",
    "print(confusion_matrix(y_test,pred))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hpsklearn import HyperoptEstimator, sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 11.49trial/s, best loss: 0.06511627906976747]\n",
      "100%|██████████| 2/2 [00:00<00:00, 28.35trial/s, best loss: 0.046511627906976716]\n",
      "100%|██████████| 3/3 [00:00<00:00, 49.19trial/s, best loss: 0.046511627906976716]\n",
      "100%|██████████| 4/4 [00:00<00:00, 59.73trial/s, best loss: 0.046511627906976716]\n",
      "100%|██████████| 5/5 [00:00<00:00, 78.05trial/s, best loss: 0.046511627906976716]\n",
      "100%|██████████| 6/6 [00:00<00:00, 86.17trial/s, best loss: 0.046511627906976716]\n",
      "100%|██████████| 7/7 [00:00<00:00, 114.79trial/s, best loss: 0.046511627906976716]\n",
      "100%|██████████| 8/8 [00:00<00:00, 118.37trial/s, best loss: 0.046511627906976716]\n",
      "100%|██████████| 9/9 [00:00<00:00, 110.63trial/s, best loss: 0.046511627906976716]\n",
      "100%|██████████| 10/10 [00:00<00:00, 144.34trial/s, best loss: 0.046511627906976716]\n",
      "100%|██████████| 11/11 [00:00<00:00, 113.32trial/s, best loss: 0.046511627906976716]\n",
      "100%|██████████| 12/12 [00:00<00:00, 190.24trial/s, best loss: 0.046511627906976716]\n",
      "100%|██████████| 13/13 [00:00<00:00, 232.54trial/s, best loss: 0.046511627906976716]\n",
      "100%|██████████| 14/14 [00:00<00:00, 218.35trial/s, best loss: 0.046511627906976716]\n",
      "100%|██████████| 15/15 [00:00<00:00, 56.04trial/s, best loss: 0.046511627906976716]\n",
      "100%|██████████| 16/16 [00:00<00:00, 130.30trial/s, best loss: 0.046511627906976716]\n",
      "100%|██████████| 17/17 [00:00<00:00, 210.65trial/s, best loss: 0.046511627906976716]\n",
      "100%|██████████| 18/18 [00:00<00:00, 137.60trial/s, best loss: 0.046511627906976716]\n",
      "100%|██████████| 19/19 [00:00<00:00, 305.31trial/s, best loss: 0.046511627906976716]\n",
      "100%|██████████| 20/20 [00:00<00:00, 258.62trial/s, best loss: 0.046511627906976716]\n",
      "100%|██████████| 21/21 [00:00<00:00, 32.04trial/s, best loss: 0.046511627906976716]\n",
      "100%|██████████| 22/22 [00:00<00:00, 198.06trial/s, best loss: 0.046511627906976716]\n",
      "100%|██████████| 23/23 [00:00<00:00, 194.99trial/s, best loss: 0.046511627906976716]\n",
      "100%|██████████| 24/24 [00:00<00:00, 45.29trial/s, best loss: 0.046511627906976716]\n",
      "100%|██████████| 25/25 [00:00<00:00, 234.86trial/s, best loss: 0.046511627906976716]\n",
      "100%|██████████| 26/26 [00:00<00:00, 182.06trial/s, best loss: 0.046511627906976716]\n",
      "100%|██████████| 27/27 [00:01<00:00, 26.25trial/s, best loss: 0.046511627906976716]\n",
      "100%|██████████| 28/28 [00:00<00:00, 74.74trial/s, best loss: 0.046511627906976716]\n",
      "100%|██████████| 29/29 [00:00<00:00, 295.67trial/s, best loss: 0.046511627906976716]\n",
      "100%|██████████| 30/30 [00:00<00:00, 259.22trial/s, best loss: 0.046511627906976716]\n",
      "100%|██████████| 31/31 [00:00<00:00, 417.85trial/s, best loss: 0.046511627906976716]\n",
      "100%|██████████| 32/32 [00:00<00:00, 218.78trial/s, best loss: 0.046511627906976716]\n",
      "100%|██████████| 33/33 [00:00<00:00, 212.49trial/s, best loss: 0.046511627906976716]\n",
      "100%|██████████| 34/34 [00:00<00:00, 445.45trial/s, best loss: 0.046511627906976716]\n",
      "100%|██████████| 35/35 [00:00<00:00, 504.96trial/s, best loss: 0.046511627906976716]\n",
      "100%|██████████| 36/36 [00:00<00:00, 452.97trial/s, best loss: 0.046511627906976716]\n",
      "100%|██████████| 37/37 [00:00<00:00, 490.33trial/s, best loss: 0.046511627906976716]\n",
      "100%|██████████| 38/38 [00:00<00:00, 527.59trial/s, best loss: 0.046511627906976716]\n",
      "100%|██████████| 39/39 [00:00<00:00, 463.70trial/s, best loss: 0.046511627906976716]\n",
      "100%|██████████| 40/40 [00:00<00:00, 453.41trial/s, best loss: 0.046511627906976716]\n",
      "100%|██████████| 41/41 [00:00<00:00, 436.45trial/s, best loss: 0.046511627906976716]\n",
      "100%|██████████| 42/42 [00:00<00:00, 101.07trial/s, best loss: 0.046511627906976716]\n",
      "100%|██████████| 43/43 [00:00<00:00, 626.03trial/s, best loss: 0.046511627906976716]\n",
      "100%|██████████| 44/44 [00:00<00:00, 402.95trial/s, best loss: 0.046511627906976716]\n",
      "100%|██████████| 45/45 [00:00<00:00, 283.03trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 46/46 [00:00<00:00, 459.51trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 47/47 [00:00<00:00, 405.41trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 48/48 [00:00<00:00, 260.47trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 49/49 [00:00<00:00, 449.33trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 50/50 [00:00<00:00, 380.34trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 51/51 [00:00<00:00, 446.83trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 52/52 [00:00<00:00, 550.17trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 53/53 [00:00<00:00, 616.08trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 54/54 [00:00<00:00, 242.86trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 55/55 [00:00<00:00, 555.98trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 56/56 [00:00<00:00, 667.86trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 57/57 [00:00<00:00, 619.78trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 58/58 [00:00<00:00, 513.45trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 59/59 [00:00<00:00, 131.14trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 60/60 [00:00<00:00, 589.08trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 61/61 [00:00<00:00, 545.65trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 62/62 [00:00<00:00, 698.17trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 63/63 [00:00<00:00, 539.34trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 64/64 [00:00<00:00, 584.38trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 65/65 [00:00<00:00, 333.27trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 66/66 [00:00<00:00, 524.42trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 67/67 [00:00<00:00, 679.78trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 68/68 [00:00<00:00, 809.37trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 69/69 [00:00<00:00, 825.94trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 70/70 [00:00<00:00, 685.94trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 71/71 [00:00<00:00, 558.08trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 72/72 [00:00<00:00, 677.73trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 73/73 [00:00<00:00, 663.82trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 74/74 [00:00<00:00, 990.86trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 75/75 [00:00<00:00, 881.01trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 76/76 [00:00<00:00, 668.36trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 77/77 [00:00<00:00, 862.51trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 78/78 [00:00<00:00, 1000.02trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 79/79 [00:00<00:00, 944.35trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 80/80 [00:00<00:00, 823.17trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 81/81 [00:00<00:00, 204.91trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 82/82 [00:00<00:00, 240.51trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 83/83 [00:00<00:00, 704.13trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 84/84 [00:00<00:00, 646.25trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 85/85 [00:00<00:00, 1070.42trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 86/86 [00:00<00:00, 673.95trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 87/87 [00:00<00:00, 719.03trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 88/88 [00:00<00:00, 259.33trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 89/89 [00:00<00:00, 648.40trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 90/90 [00:00<00:00, 856.12trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 91/91 [00:00<00:00, 736.31trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 92/92 [00:00<00:00, 747.86trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 93/93 [00:00<00:00, 791.23trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 94/94 [00:00<00:00, 445.24trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 95/95 [00:00<00:00, 705.56trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 96/96 [00:00<00:00, 760.46trial/s, best loss: 0.041860465116279055]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97/97 [00:00<00:00, 863.54trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 98/98 [00:00<00:00, 379.54trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 99/99 [00:00<00:00, 498.95trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 100/100 [00:00<00:00, 709.84trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 101/101 [00:00<00:00, 654.33trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 102/102 [00:00<00:00, 355.02trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 103/103 [00:00<00:00, 866.87trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 104/104 [00:00<00:00, 622.07trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 105/105 [00:00<00:00, 913.97trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 106/106 [00:00<00:00, 206.50trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 107/107 [00:00<00:00, 1127.01trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 108/108 [00:00<00:00, 872.68trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 109/109 [00:00<00:00, 184.23trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 110/110 [00:00<00:00, 113.12trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 111/111 [00:00<00:00, 895.37trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 112/112 [00:00<00:00, 747.16trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 113/113 [00:00<00:00, 1014.19trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 114/114 [00:00<00:00, 837.55trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 115/115 [00:00<00:00, 1206.32trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 116/116 [00:00<00:00, 425.60trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 117/117 [00:00<00:00, 828.21trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 118/118 [00:00<00:00, 984.56trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 119/119 [00:00<00:00, 292.99trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 120/120 [00:00<00:00, 606.98trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 121/121 [00:00<00:00, 952.79trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 122/122 [00:00<00:00, 769.11trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 123/123 [00:00<00:00, 1337.80trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 124/124 [00:00<00:00, 1138.16trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 125/125 [00:00<00:00, 957.39trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 126/126 [00:00<00:00, 1097.06trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 127/127 [00:00<00:00, 1443.96trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 128/128 [00:00<00:00, 1066.74trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 129/129 [00:00<00:00, 909.11trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 130/130 [00:00<00:00, 940.95trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 131/131 [00:00<00:00, 351.02trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 132/132 [00:00<00:00, 939.68trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 133/133 [00:00<00:00, 1168.63trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 134/134 [00:00<00:00, 1260.88trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 135/135 [00:00<00:00, 1015.75trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 136/136 [00:00<00:00, 1102.47trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 137/137 [00:00<00:00, 1698.07trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 138/138 [00:00<00:00, 1009.66trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 139/139 [00:00<00:00, 1175.50trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 140/140 [00:00<00:00, 254.49trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 141/141 [00:00<00:00, 664.84trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 142/142 [00:00<00:00, 729.42trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 143/143 [00:00<00:00, 470.61trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 144/144 [00:00<00:00, 968.18trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 145/145 [00:00<00:00, 1137.50trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 146/146 [00:00<00:00, 1256.46trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 147/147 [00:00<00:00, 1134.98trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 148/148 [00:00<00:00, 1167.30trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 149/149 [00:00<00:00, 1098.50trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 150/150 [00:00<00:00, 193.67trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 151/151 [00:00<00:00, 473.79trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 152/152 [00:00<00:00, 888.87trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 153/153 [00:00<00:00, 1076.87trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 154/154 [00:00<00:00, 1186.35trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 155/155 [00:00<00:00, 1237.05trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 156/156 [00:00<00:00, 1205.26trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 157/157 [00:00<00:00, 1428.08trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 158/158 [00:00<00:00, 1286.67trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 159/159 [00:00<00:00, 1231.30trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 160/160 [00:00<00:00, 1118.54trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 161/161 [00:00<00:00, 1693.95trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 162/162 [00:00<00:00, 1571.42trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 163/163 [00:00<00:00, 1489.85trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 164/164 [00:00<00:00, 1168.15trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 165/165 [00:00<00:00, 651.83trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 166/166 [00:00<00:00, 1263.69trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 167/167 [00:00<00:00, 1181.08trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 168/168 [00:00<00:00, 1757.71trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 169/169 [00:00<00:00, 1935.19trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 170/170 [00:00<00:00, 1294.24trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 171/171 [00:02<00:00, 57.64trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 172/172 [00:00<00:00, 861.61trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 173/173 [00:00<00:00, 1380.04trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 174/174 [00:00<00:00, 1437.69trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 175/175 [00:00<00:00, 1291.65trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 176/176 [00:00<00:00, 1276.07trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 177/177 [00:00<00:00, 1350.44trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 178/178 [00:00<00:00, 1863.02trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 179/179 [00:00<00:00, 1308.58trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 180/180 [00:00<00:00, 1430.74trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 181/181 [00:00<00:00, 700.76trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 182/182 [00:00<00:00, 483.00trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 183/183 [00:00<00:00, 1800.27trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 184/184 [00:00<00:00, 650.82trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 185/185 [00:00<00:00, 1404.64trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 186/186 [00:00<00:00, 594.36trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 187/187 [00:00<00:00, 929.50trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 188/188 [00:00<00:00, 1534.99trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 189/189 [00:00<00:00, 1775.85trial/s, best loss: 0.041860465116279055]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 190/190 [00:00<00:00, 2017.94trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 191/191 [00:00<00:00, 1498.82trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 192/192 [00:00<00:00, 1335.61trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 193/193 [00:00<00:00, 1389.48trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 194/194 [00:00<00:00, 1439.03trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 195/195 [00:00<00:00, 1821.72trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 196/196 [00:00<00:00, 869.19trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 197/197 [00:00<00:00, 1424.55trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 198/198 [00:00<00:00, 391.74trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 199/199 [00:00<00:00, 1187.83trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 200/200 [00:00<00:00, 316.61trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 201/201 [00:00<00:00, 1678.52trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 202/202 [00:00<00:00, 1564.26trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 203/203 [00:00<00:00, 1493.68trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1477.24trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 205/205 [00:00<00:00, 1429.08trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 206/206 [00:00<00:00, 1514.36trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 207/207 [00:00<00:00, 438.88trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 208/208 [00:00<00:00, 2124.17trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 209/209 [00:00<00:00, 1533.96trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 210/210 [00:00<00:00, 1986.17trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 211/211 [00:00<00:00, 1399.38trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 212/212 [00:00<00:00, 1336.97trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 213/213 [00:00<00:00, 2374.07trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 214/214 [00:00<00:00, 1434.88trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 215/215 [00:00<00:00, 914.66trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1578.85trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 217/217 [00:00<00:00, 1714.29trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 218/218 [00:00<00:00, 1754.09trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 219/219 [00:00<00:00, 2346.59trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 220/220 [00:00<00:00, 2312.56trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 221/221 [00:00<00:00, 1957.05trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 222/222 [00:00<00:00, 1999.09trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 223/223 [00:00<00:00, 1706.62trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 224/224 [00:00<00:00, 973.88trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 225/225 [00:00<00:00, 1707.54trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 226/226 [00:00<00:00, 2177.19trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 227/227 [00:00<00:00, 1675.96trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 228/228 [00:00<00:00, 799.85trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 229/229 [00:00<00:00, 671.04trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 230/230 [00:00<00:00, 1623.90trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 231/231 [00:00<00:00, 2335.56trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 232/232 [00:00<00:00, 1003.94trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 233/233 [00:00<00:00, 2019.99trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 234/234 [00:00<00:00, 1593.53trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 235/235 [00:00<00:00, 2019.36trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 236/236 [00:00<00:00, 1719.94trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 237/237 [00:00<00:00, 1769.86trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 238/238 [00:00<00:00, 2086.78trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 239/239 [00:00<00:00, 1507.17trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 240/240 [00:00<00:00, 1705.66trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 241/241 [00:00<00:00, 1768.72trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 242/242 [00:00<00:00, 1947.00trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 243/243 [00:00<00:00, 2638.80trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 244/244 [00:00<00:00, 1456.03trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 245/245 [00:00<00:00, 1799.85trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 246/246 [00:00<00:00, 2045.81trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 247/247 [00:00<00:00, 1628.63trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 248/248 [00:00<00:00, 1322.13trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 249/249 [00:00<00:00, 1402.76trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 250/250 [00:00<00:00, 477.58trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 251/251 [00:00<00:00, 1889.57trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 252/252 [00:00<00:00, 1760.59trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 253/253 [00:00<00:00, 1737.55trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 254/254 [00:00<00:00, 1426.69trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 255/255 [00:00<00:00, 2036.52trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 256/256 [00:00<00:00, 340.15trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 257/257 [00:00<00:00, 1991.74trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 258/258 [00:00<00:00, 1480.31trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 259/259 [00:00<00:00, 1927.98trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 260/260 [00:00<00:00, 1982.84trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 261/261 [00:00<00:00, 1721.87trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 262/262 [00:00<00:00, 1953.94trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 263/263 [00:00<00:00, 527.54trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 264/264 [00:00<00:00, 1493.84trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 265/265 [00:00<00:00, 1930.46trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 266/266 [00:00<00:00, 2253.43trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 267/267 [00:00<00:00, 2269.15trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 268/268 [00:00<00:00, 1947.88trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 269/269 [00:00<00:00, 1945.82trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 270/270 [00:00<00:00, 1919.22trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 271/271 [00:00<00:00, 551.12trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 272/272 [00:00<00:00, 1954.73trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 273/273 [00:00<00:00, 1793.39trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 274/274 [00:00<00:00, 1830.31trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 275/275 [00:00<00:00, 2045.99trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 276/276 [00:00<00:00, 1781.57trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 277/277 [00:00<00:00, 1630.14trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 278/278 [00:00<00:00, 1844.90trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 279/279 [00:00<00:00, 1505.09trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 280/280 [00:00<00:00, 924.31trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 281/281 [00:00<00:00, 1666.33trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 282/282 [00:00<00:00, 2098.27trial/s, best loss: 0.041860465116279055]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 283/283 [00:00<00:00, 1728.59trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 284/284 [00:01<00:00, 241.01trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 285/285 [00:00<00:00, 1771.42trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 286/286 [00:00<00:00, 1915.41trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 287/287 [00:00<00:00, 417.66trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 288/288 [00:00<00:00, 2039.88trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 289/289 [00:00<00:00, 2640.21trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 290/290 [00:00<00:00, 2058.35trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 291/291 [00:00<00:00, 2311.31trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 292/292 [00:00<00:00, 2099.28trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 293/293 [00:00<00:00, 729.76trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 294/294 [00:00<00:00, 2176.04trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 295/295 [00:00<00:00, 2327.81trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 296/296 [00:00<00:00, 2147.98trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 297/297 [00:00<00:00, 1042.80trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 298/298 [00:00<00:00, 1034.40trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 299/299 [00:00<00:00, 2032.85trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 300/300 [00:00<00:00, 2675.64trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 301/301 [00:00<00:00, 1917.06trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 302/302 [00:00<00:00, 1898.78trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 303/303 [00:00<00:00, 2183.83trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 304/304 [00:00<00:00, 2112.81trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 305/305 [00:00<00:00, 355.66trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 306/306 [00:00<00:00, 2202.33trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 307/307 [00:00<00:00, 2073.66trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 308/308 [00:00<00:00, 720.20trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 309/309 [00:00<00:00, 2025.05trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 310/310 [00:00<00:00, 2604.66trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 311/311 [00:00<00:00, 1953.01trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 312/312 [00:00<00:00, 1081.43trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 313/313 [00:00<00:00, 753.90trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 314/314 [00:00<00:00, 3185.67trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 315/315 [00:00<00:00, 2121.60trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 316/316 [00:00<00:00, 2249.17trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 317/317 [00:01<00:00, 262.45trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 318/318 [00:00<00:00, 1153.20trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 319/319 [00:00<00:00, 1569.12trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 320/320 [00:00<00:00, 2376.84trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 321/321 [00:00<00:00, 2173.68trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 322/322 [00:00<00:00, 1803.03trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 323/323 [00:00<00:00, 2179.64trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 324/324 [00:00<00:00, 589.74trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 325/325 [00:00<00:00, 936.24trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 326/326 [00:00<00:00, 2296.28trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 327/327 [00:00<00:00, 1614.46trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 328/328 [00:00<00:00, 2487.52trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 329/329 [00:00<00:00, 2850.24trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 330/330 [00:00<00:00, 2458.69trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 331/331 [00:00<00:00, 1963.55trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 332/332 [00:00<00:00, 2731.52trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 333/333 [00:00<00:00, 2174.64trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 334/334 [00:00<00:00, 2561.11trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 335/335 [00:00<00:00, 2422.84trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 336/336 [00:00<00:00, 2287.91trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 337/337 [00:00<00:00, 2697.04trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 338/338 [00:00<00:00, 2698.29trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 339/339 [00:00<00:00, 2157.09trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 340/340 [00:00<00:00, 2762.45trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 341/341 [00:00<00:00, 3023.82trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 342/342 [00:00<00:00, 1074.40trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 343/343 [00:00<00:00, 540.09trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 344/344 [00:00<00:00, 1972.62trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 345/345 [00:00<00:00, 2240.43trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 346/346 [00:00<00:00, 2345.55trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 347/347 [00:00<00:00, 2324.78trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 348/348 [00:00<00:00, 3068.09trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 349/349 [00:00<00:00, 3327.43trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 350/350 [00:00<00:00, 2909.15trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 351/351 [00:00<00:00, 1519.33trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 352/352 [00:00<00:00, 1484.81trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 353/353 [00:00<00:00, 2828.03trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 354/354 [00:00<00:00, 2968.26trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 355/355 [00:00<00:00, 2207.17trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 356/356 [00:00<00:00, 2606.03trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 357/357 [00:00<00:00, 2726.72trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 358/358 [00:00<00:00, 2642.39trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 359/359 [00:00<00:00, 3351.54trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 360/360 [00:00<00:00, 438.34trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 361/361 [00:00<00:00, 2537.60trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 362/362 [00:00<00:00, 1456.78trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 363/363 [00:00<00:00, 2654.05trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 364/364 [00:00<00:00, 2233.46trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 365/365 [00:00<00:00, 2459.86trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 366/366 [00:00<00:00, 2642.65trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 367/367 [00:00<00:00, 2331.06trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 368/368 [00:00<00:00, 612.16trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 369/369 [00:00<00:00, 2472.92trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 370/370 [00:00<00:00, 3061.99trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 371/371 [00:00<00:00, 2561.12trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 372/372 [00:00<00:00, 2695.08trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 373/373 [00:00<00:00, 3247.94trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 374/374 [00:00<00:00, 3545.22trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 375/375 [00:00<00:00, 3131.47trial/s, best loss: 0.041860465116279055]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 376/376 [00:00<00:00, 3503.13trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 377/377 [00:00<00:00, 3335.05trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 378/378 [00:00<00:00, 2411.78trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 379/379 [00:00<00:00, 3086.75trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 380/380 [00:00<00:00, 3075.39trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 381/381 [00:00<00:00, 2775.76trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 382/382 [00:00<00:00, 2663.11trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 383/383 [00:00<00:00, 2200.76trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 384/384 [00:00<00:00, 3262.46trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 385/385 [00:00<00:00, 2712.18trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 386/386 [00:00<00:00, 2601.24trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 387/387 [00:00<00:00, 2291.53trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 388/388 [00:00<00:00, 2544.18trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 389/389 [00:00<00:00, 3208.21trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 390/390 [00:00<00:00, 3323.31trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 391/391 [00:00<00:00, 3056.21trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 392/392 [00:00<00:00, 2746.17trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 393/393 [00:00<00:00, 3073.33trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 394/394 [00:00<00:00, 2391.42trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 395/395 [00:00<00:00, 3333.05trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 396/396 [00:00<00:00, 2439.59trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 397/397 [00:00<00:00, 2599.29trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 398/398 [00:00<00:00, 1093.60trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 399/399 [00:00<00:00, 3309.90trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 400/400 [00:00<00:00, 2580.31trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 401/401 [00:00<00:00, 2819.73trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 402/402 [00:00<00:00, 2554.70trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 403/403 [00:00<00:00, 2353.38trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 404/404 [00:00<00:00, 2871.26trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 405/405 [00:00<00:00, 2476.61trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 406/406 [00:00<00:00, 1254.93trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 407/407 [00:00<00:00, 2728.78trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 408/408 [00:00<00:00, 2604.72trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 409/409 [00:00<00:00, 1217.73trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 410/410 [00:00<00:00, 1493.08trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 411/411 [00:00<00:00, 2830.42trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 412/412 [00:00<00:00, 2571.23trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 413/413 [00:00<00:00, 2297.35trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 414/414 [00:00<00:00, 2560.20trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 415/415 [00:00<00:00, 1443.96trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 416/416 [00:00<00:00, 2237.52trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 417/417 [00:00<00:00, 2463.25trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 418/418 [00:00<00:00, 2823.81trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 419/419 [00:00<00:00, 1319.61trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 420/420 [00:00<00:00, 1775.02trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 421/421 [00:00<00:00, 2595.66trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 422/422 [00:00<00:00, 1727.85trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 423/423 [00:00<00:00, 1807.94trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 424/424 [00:00<00:00, 2559.08trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 425/425 [00:00<00:00, 3531.65trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 426/426 [00:00<00:00, 2575.09trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 427/427 [00:00<00:00, 1234.80trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 428/428 [00:00<00:00, 1770.33trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 429/429 [00:00<00:00, 2666.21trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 430/430 [00:00<00:00, 3195.15trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 431/431 [00:00<00:00, 2863.85trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 432/432 [00:00<00:00, 2056.44trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 433/433 [00:00<00:00, 2863.37trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 434/434 [00:00<00:00, 2476.01trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 435/435 [00:00<00:00, 1410.74trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 436/436 [00:00<00:00, 3192.46trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 437/437 [00:00<00:00, 2914.01trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 438/438 [00:00<00:00, 1957.44trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 439/439 [00:00<00:00, 3145.12trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 440/440 [00:00<00:00, 2501.99trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 441/441 [00:00<00:00, 2873.83trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 442/442 [00:00<00:00, 1176.25trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 443/443 [00:00<00:00, 2679.42trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 444/444 [00:00<00:00, 2451.20trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 445/445 [00:00<00:00, 2554.76trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 446/446 [00:00<00:00, 2850.49trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 447/447 [00:00<00:00, 2062.30trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 448/448 [00:00<00:00, 3137.11trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 449/449 [00:00<00:00, 1998.29trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 450/450 [00:00<00:00, 3048.39trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 451/451 [00:00<00:00, 2825.79trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 452/452 [00:00<00:00, 2669.60trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 453/453 [00:00<00:00, 2699.83trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 454/454 [00:00<00:00, 2132.12trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 455/455 [00:00<00:00, 2237.88trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 456/456 [00:00<00:00, 3597.73trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 457/457 [00:00<00:00, 3457.79trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 458/458 [00:00<00:00, 2530.89trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 459/459 [00:00<00:00, 3465.09trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 460/460 [00:00<00:00, 950.25trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 461/461 [00:00<00:00, 2865.92trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 462/462 [00:00<00:00, 1413.56trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 463/463 [00:00<00:00, 3216.03trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 464/464 [00:00<00:00, 2086.93trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 465/465 [00:00<00:00, 3077.42trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 466/466 [00:00<00:00, 2313.38trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 467/467 [00:00<00:00, 3625.52trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 468/468 [00:00<00:00, 2807.56trial/s, best loss: 0.037209302325581395]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [00:00<00:00, 3254.62trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 470/470 [00:00<00:00, 2293.07trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 471/471 [00:00<00:00, 2776.46trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 472/472 [00:00<00:00, 3149.75trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 473/473 [00:00<00:00, 2940.35trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 474/474 [00:00<00:00, 3492.77trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 475/475 [00:00<00:00, 2861.09trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 476/476 [00:00<00:00, 2736.12trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 477/477 [00:00<00:00, 2840.64trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 478/478 [00:00<00:00, 2972.47trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 479/479 [00:00<00:00, 3624.04trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 480/480 [00:00<00:00, 1753.62trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 481/481 [00:00<00:00, 3115.87trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 482/482 [00:00<00:00, 868.25trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 483/483 [00:00<00:00, 3834.24trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 484/484 [00:00<00:00, 3337.65trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 485/485 [00:00<00:00, 2951.67trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 486/486 [00:00<00:00, 3618.74trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 487/487 [00:00<00:00, 1217.78trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 488/488 [00:00<00:00, 2857.08trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 489/489 [00:00<00:00, 2911.85trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 490/490 [00:00<00:00, 989.42trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 491/491 [00:00<00:00, 2558.43trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 492/492 [00:00<00:00, 712.12trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 493/493 [00:00<00:00, 2909.84trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 494/494 [00:00<00:00, 2653.43trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 495/495 [00:00<00:00, 3516.46trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 496/496 [00:00<00:00, 1983.05trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 497/497 [00:00<00:00, 3129.13trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 498/498 [00:00<00:00, 2478.17trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 499/499 [00:00<00:00, 2145.31trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 500/500 [00:00<00:00, 2681.13trial/s, best loss: 0.037209302325581395]\n",
      "0.9264069264069265\n",
      "{'learner': SGDClassifier(alpha=3.5551124276058275e-05, average=False,\n",
      "              class_weight='balanced', early_stopping=False, epsilon=0.1,\n",
      "              eta0=0.057615032359113263, fit_intercept=True,\n",
      "              l1_ratio=0.5016334128226829, learning_rate='constant',\n",
      "              loss='modified_huber', max_iter=102539159.0, n_iter_no_change=5,\n",
      "              n_jobs=1, penalty='l1', power_t=0.6259697404380353,\n",
      "              random_state=4, shuffle=True, tol=0.0012888409860024497,\n",
      "              validation_fraction=0.1, verbose=False, warm_start=False), 'preprocs': (), 'ex_preprocs': ()}\n",
      "accuracy: 0.9264069264069265\n",
      "f1: 0.8768115942028986\n",
      "roc_auc: 0.9394896142433236\n",
      "recall: 0.968\n",
      "precision: 0.8013245033112583\n",
      "[[307  30]\n",
      " [  4 121]]\n"
     ]
    }
   ],
   "source": [
    "AutobayesianMetrics = {}\n",
    "Optimisers['Auto Bayesian'] = AutobayesianMetrics\n",
    "\n",
    "estim = HyperoptEstimator(classifier=sgd('sgd'),\n",
    "                          preprocessing=[],\n",
    "                          algo=tpe.suggest,\n",
    "                          max_evals=500,\n",
    "                          trial_timeout=120)\n",
    "\n",
    "# Search the hyperparameter space based on the data\n",
    "estim.fit(X_train.values, y_train)\n",
    "\n",
    "# Show the results\n",
    "\n",
    "print(estim.score(X_test.values, y_test))\n",
    "# 1.0\n",
    "\n",
    "print( estim.best_model() )\n",
    "\n",
    "model = estim.best_model()\n",
    "trainedsgd = model['learner']\n",
    "pred = trainedsgd.predict(X_test)\n",
    "\n",
    "AutobayesianMetrics['accuracy'] = accuracy_score(y_test, pred)\n",
    "AutobayesianMetrics['f1'] = f1_score(y_test, pred)\n",
    "AutobayesianMetrics['roc_auc'] = roc_auc_score(y_test, pred)\n",
    "AutobayesianMetrics['recall'] = recall_score(y_test, pred)\n",
    "AutobayesianMetrics['precision'] = precision_score(y_test, pred)\n",
    "\n",
    "for metric, score in AutobayesianMetrics.items():\n",
    "    print(f'{metric}: {score}')\n",
    "    \n",
    "print(confusion_matrix(y_test,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[311  26]\n",
      " [  6 119]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.92      0.95       337\n",
      "           1       0.82      0.95      0.88       125\n",
      "\n",
      "    accuracy                           0.93       462\n",
      "   macro avg       0.90      0.94      0.92       462\n",
      "weighted avg       0.94      0.93      0.93       462\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = estim.best_model()\n",
    "trainedsgd = model['learner']\n",
    "predictionforest = trainedforest.predict(X_test)\n",
    "print(confusion_matrix(y_test,predictionforest))\n",
    "print(classification_report(y_test,predictionforest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tpot import TPOTClassifier\n",
    "\n",
    "\n",
    "GeneticMetrics = {}\n",
    "Optimisers['Genetic'] = GeneticMetrics\n",
    "\n",
    "tpot_config = {\n",
    "    'sklearn.linear_model.SGDClassifier': {\n",
    "        'loss': ['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron'],\n",
    "        'penalty': ['l2', 'l1', 'elasticnet'],\n",
    "        'alpha': [0.0001, 0.001, 0.01],\n",
    "        'learning_rate': ['invscaling', 'constant', 'optimal'],\n",
    "        'fit_intercept': [True, False],\n",
    "        'l1_ratio': [0.15, 0.25, 0.0, 1.0, 0.75, 0.5],\n",
    "        'eta0': [0.01, 0.1, 1.0],\n",
    "        'power_t': [0.5, 0.0, 1.0, 0.1, 100.0, 10.0, 50.0]\n",
    "    }\n",
    "}\n",
    "        \n",
    "tpot = TPOTClassifier(generations= 100, population_size= 100,\n",
    "                                 verbosity= 2, n_jobs=-1, config_dict=tpot_config)\n",
    "tpot.fit(X_train, y_train)\n",
    "pred = tpot.predict(X_test)\n",
    "print(tpot.score(X_test, y_test))\n",
    "print(tpot.fitted_pipeline_)\n",
    "\n",
    "\n",
    "GeneticMetrics['accuracy'] = accuracy_score(y_test, pred)\n",
    "GeneticMetrics['f1'] = f1_score(y_test, pred)\n",
    "GeneticMetrics['roc_auc'] = roc_auc_score(y_test, pred)\n",
    "GeneticMetrics['recall'] = recall_score(y_test, pred)\n",
    "GeneticMetrics['precision'] = precision_score(y_test, pred)\n",
    "\n",
    "for metric, score in GeneticMetrics.items():\n",
    "    print(f'{metric}: {score}')\n",
    "    \n",
    "print(confusion_matrix(y_test,pred))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for optmimiser, metrics in Optimisers.items():\n",
    "    print(f'\\n****{optmimiser}****')\n",
    "    for metric, score in metrics.items():\n",
    "        print(f'{metric}: {score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bio-Inspired Algorithms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opytimizer import Opytimizer\n",
    "from opytimizer.core.function import Function\n",
    "from opytimizer.optimizers.pso import PSO\n",
    "from opytimizer.spaces.search import SearchSpace\n",
    "from sklearn.model_selection import KFold, cross_val_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gathering hyperparams\n",
    "def _SGD(opytimizer):\n",
    "    # Gathering hyperparams\n",
    "    loss_ = {0: 'hinge', 1: 'log', 2: 'modified_huber', 3: 'squared_hinge', 4: 'perceptron'}\n",
    "    penalty_ = {0: 'l2', 1: 'l1', 2: 'elasticnet'}\n",
    "    learning_rate_ = {0: 'invscaling', 1: 'constant'}\n",
    "    fit_intercept_ = {0: True, 1: False}\n",
    "    \n",
    "    loss = loss_[int(opytimizer[0][0])]\n",
    "    penalty = penalty_[int(opytimizer[1][0])]\n",
    "    alpha = opytimizer[2][0]\n",
    "    learning_rate = learning_rate_[int(opytimizer[3][0])]\n",
    "    fit_intercept = fit_intercept_[int(opytimizer[4][0])]\n",
    "    l1_ratio =  opytimizer[5][0]\n",
    "    eta0 = opytimizer[6][0]\n",
    "    power_t = opytimizer[7][0]\n",
    "    \n",
    "    \n",
    "    # Instanciating an SVC class\n",
    "    model = SGDClassifier(loss = loss, \n",
    "                          penalty = penalty,\n",
    "                          alpha = alpha,\n",
    "                          learning_rate = learning_rate,\n",
    "                          fit_intercept = fit_intercept,\n",
    "                          l1_ratio = l1_ratio,\n",
    "                          eta0 = eta0, \n",
    "                          power_t = power_t\n",
    "                         )\n",
    "\n",
    "\n",
    "    # Fitting model using cross-validation\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=5, n_jobs=-1)\n",
    "    \n",
    "    # Calculating scores mean\n",
    "    mean_score = np.mean(scores)\n",
    "\n",
    "    return 1 - mean_score\n",
    "\n",
    "f = Function(pointer=_SGD)\n",
    "\n",
    "# Number of agents\n",
    "n_agents = 14\n",
    "\n",
    "# Number of decision variables\n",
    "n_variables = 8\n",
    "\n",
    "# Number of running iterations\n",
    "n_iterations = 200\n",
    "\n",
    "                       \n",
    "    \n",
    "# Lower and upper bounds (has to be the same size as n_variables)\n",
    "lower_bound = [0, 0, 0.0001, 0, 0, 0.15, 0.01, 0.0]\n",
    "upper_bound = [4, 2, 0.01, 1, 1, 1.0, 1.0, 100.0]\n",
    "\n",
    "s = SearchSpace(n_agents=n_agents, n_iterations=n_iterations,\n",
    "                n_variables=n_variables, lower_bound=lower_bound,\n",
    "                upper_bound=upper_bound)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for the optimizer\n",
    "hyperparams = {\n",
    "    'w': 0.7,\n",
    "    'c1': 1.7,\n",
    "    'c2': 1.7\n",
    "}\n",
    "\n",
    "# Finally, we can create an Opytimizer class\n",
    "p = PSO(hyperparams=hyperparams)\n",
    "PSO_opitmise = Opytimizer(space=s, optimizer=p, function=f)\n",
    "\n",
    "# Running the optimization task\n",
    "history = PSO_opitmise.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = np.array(history.best_agent[-1][0]).ravel()\n",
    "\n",
    "bioOptimisers = {}\n",
    "PSOMetrics = {}\n",
    "bioOptimisers['Partical Swarm'] = PSOMetrics\n",
    "\n",
    "\n",
    "loss_ = {0: 'hinge', 1: 'log', 2: 'modified_huber', 3: 'squared_hinge', 4: 'perceptron'}\n",
    "penalty_ = {0: 'l2', 1: 'l1', 2: 'elasticnet'}\n",
    "learning_rate_ = {0: 'invscaling', 1: 'constant'}\n",
    "fit_intercept_ = {0: True, 1: False}\n",
    "\n",
    "\n",
    "sgd = SGDClassifier(loss = loss_[int(best[0])], \n",
    "                  penalty = penalty_[int(best[1])], \n",
    "                  alpha = best[2], \n",
    "                  learning_rate = learning_rate_[int(best[3])], \n",
    "                  fit_intercept = fit_intercept_[int(best[4])], \n",
    "                  l1_ratio = best[5],\n",
    "                  eta0 = best[6],\n",
    "                  power_t = best[7]).fit(X_train, y_train)\n",
    "\n",
    "                              \n",
    "pred = sgd.predict(X_test)\n",
    "\n",
    "\n",
    "PSOMetrics['accuracy'] = accuracy_score(y_test, pred)\n",
    "PSOMetrics['f1'] = f1_score(y_test, pred)\n",
    "PSOMetrics['roc_auc'] = roc_auc_score(y_test, pred)\n",
    "PSOMetrics['recall'] = recall_score(y_test, pred)\n",
    "PSOMetrics['precision'] = precision_score(y_test, pred)\n",
    "\n",
    "for metric, score in PSOMetrics.items():\n",
    "    print(f'{metric}: {score}')\n",
    "    \n",
    "print(confusion_matrix(y_test,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from opytimizer.optimizers.abc import ABC\n",
    "\n",
    "hyperparams = {\n",
    "    'n_trials': 10\n",
    "}\n",
    "\n",
    "# Creating an ABC optimizer\n",
    "p =  ABC(hyperparams=hyperparams)\n",
    "ABC_optimise = Opytimizer(space=s, optimizer=p, function=f)\n",
    "history = ABC_optimise.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = np.array(history.best_agent[-1][0]).ravel()\n",
    "\n",
    "ABCMetrics = {}\n",
    "bioOptimisers['ABC'] = ABCMetrics\n",
    "\n",
    "\n",
    "\n",
    "sgd = SGDClassifier(loss = loss_[int(best[0])], \n",
    "                  penalty = penalty_[int(best[1])], \n",
    "                  alpha = best[2], \n",
    "                  learning_rate = learning_rate_[int(best[3])], \n",
    "                  fit_intercept = fit_intercept_[int(best[4])], \n",
    "                  l1_ratio = best[5],\n",
    "                  eta0 = best[6],\n",
    "                  power_t = best[7]).fit(X_train, y_train)\n",
    "\n",
    "                              \n",
    "pred = sgd.predict(X_test)\n",
    "\n",
    "ABCMetrics['accuracy'] = accuracy_score(y_test, pred)\n",
    "ABCMetrics['f1'] = f1_score(y_test, pred)\n",
    "ABCMetrics['roc_auc'] = roc_auc_score(y_test, pred)\n",
    "ABCMetrics['recall'] = recall_score(y_test, pred)\n",
    "ABCMetrics['precision'] = precision_score(y_test, pred)\n",
    "\n",
    "for metric, score in ABCMetrics.items():\n",
    "    print(f'{metric}: {score}')\n",
    "    \n",
    "print(confusion_matrix(y_test,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from opytimizer.optimizers.fa import FA\n",
    "\n",
    "hyperparams = {\n",
    "    'alpha': 0.5,\n",
    "    'beta': 0.2,\n",
    "    'gamma': 1.0\n",
    "}\n",
    "\n",
    "# Creating a FA optimizer\n",
    "p = FA(hyperparams=hyperparams)\n",
    "\n",
    "FA_optimise = Opytimizer(space=s, optimizer=p, function=f)\n",
    "history = ABC_optimise.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = np.array(history.best_agent[-1][0]).ravel()\n",
    "\n",
    "FAMetrics = {}\n",
    "bioOptimisers['FA'] = FAMetrics\n",
    "\n",
    "\n",
    "crit = {0: 'entropy', 1: 'gini'}\n",
    "feat = {0: 'auto', 1: 'sqrt', 2: 'log2', 3: None}\n",
    "\n",
    "\n",
    "sgd = SGDClassifier(loss = loss_[int(best[0])], \n",
    "                  penalty = penalty_[int(best[1])], \n",
    "                  alpha = best[2], \n",
    "                  learning_rate = learning_rate_[int(best[3])], \n",
    "                  fit_intercept = fit_intercept_[int(best[4])], \n",
    "                  l1_ratio = best[5],\n",
    "                  eta0 = best[6],\n",
    "                  power_t = best[7]).fit(X_train, y_train)\n",
    "\n",
    "                              \n",
    "pred = sgd.predict(X_test)\n",
    "\n",
    "FAMetrics['accuracy'] = accuracy_score(y_test, pred)\n",
    "FAMetrics['f1'] = f1_score(y_test, pred)\n",
    "FAMetrics['roc_auc'] = roc_auc_score(y_test, pred)\n",
    "FAMetrics['recall'] = recall_score(y_test, pred)\n",
    "FAMetrics['precision'] = precision_score(y_test, pred)\n",
    "\n",
    "for metric, score in FAMetrics.items():\n",
    "    print(f'{metric}: {score}')\n",
    "    \n",
    "print(confusion_matrix(y_test,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
