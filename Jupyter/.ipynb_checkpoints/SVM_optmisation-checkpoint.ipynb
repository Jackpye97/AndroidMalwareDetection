{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Optmisation of SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, accuracy_score, f1_score, recall_score, precision_score, roc_auc_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set __location__ depeneing which operating system user is on \n",
    "\n",
    "if os.name == \"nt\": \n",
    "    __location__ = \"F:\\FinalYearProject\\Machine Learning\\Features\\CIC_dataset\\\\\"\n",
    "else:\n",
    "    __location__ = \"/media/jackp/JACK PYE/FinalYearProject/Machine Learning/Features/CIC_dataset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset into panda dataframe\n",
    "df = pd.read_csv(os.path.join(__location__,\"all_features.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   AbsListView  AbsListView.LayoutParams  AbsListView.MultiChoiceModeListener  \\\n",
      "0            1                         0                                    0   \n",
      "1            1                         0                                    0   \n",
      "2            0                         0                                    0   \n",
      "3            1                         0                                    0   \n",
      "4            0                         0                                    0   \n",
      "\n",
      "   AbsListView.OnScrollListener  AbsListView.RecyclerListener  \\\n",
      "0                             0                             0   \n",
      "1                             0                             0   \n",
      "2                             0                             0   \n",
      "3                             0                             0   \n",
      "4                             0                             0   \n",
      "\n",
      "   AbsListView.SelectionBoundsAdjuster  AbsoluteLayout  \\\n",
      "0                                    0               0   \n",
      "1                                    0               1   \n",
      "2                                    0               0   \n",
      "3                                    0               0   \n",
      "4                                    0               0   \n",
      "\n",
      "   AbsoluteLayout.LayoutParams  AbsoluteSizeSpan  AbsSavedState  ...  \\\n",
      "0                            0                 0              0  ...   \n",
      "1                            0                 0              0  ...   \n",
      "2                            0                 0              0  ...   \n",
      "3                            0                 1              0  ...   \n",
      "4                            0                 0              0  ...   \n",
      "\n",
      "   android.permission.WRITE_GSERVICES  android.permission.WRITE_MEDIA_STORAGE  \\\n",
      "0                                   0                                       0   \n",
      "1                                   0                                       0   \n",
      "2                                   0                                       0   \n",
      "3                                   0                                       1   \n",
      "4                                   0                                       0   \n",
      "\n",
      "   android.permission.WRITE_PROFILE  android.permission.WRITE_SECURE_SETTINGS  \\\n",
      "0                                 0                                         0   \n",
      "1                                 0                                         0   \n",
      "2                                 0                                         0   \n",
      "3                                 0                                         1   \n",
      "4                                 0                                         0   \n",
      "\n",
      "   android.permission.WRITE_SETTINGS  android.permission.WRITE_SMS  \\\n",
      "0                                  0                             0   \n",
      "1                                  0                             0   \n",
      "2                                  0                             0   \n",
      "3                                  1                             0   \n",
      "4                                  0                             0   \n",
      "\n",
      "   android.permission.WRITE_SOCIAL_STREAM  \\\n",
      "0                                       0   \n",
      "1                                       0   \n",
      "2                                       0   \n",
      "3                                       0   \n",
      "4                                       0   \n",
      "\n",
      "   android.permission.WRITE_SYNC_SETTINGS  \\\n",
      "0                                       0   \n",
      "1                                       0   \n",
      "2                                       0   \n",
      "3                                       1   \n",
      "4                                       0   \n",
      "\n",
      "   android.permission.WRITE_USER_DICTIONARY  malware  \n",
      "0                                         0   Benign  \n",
      "1                                         0  Malware  \n",
      "2                                         0  Malware  \n",
      "3                                         0  Malware  \n",
      "4                                         0   Benign  \n",
      "\n",
      "[5 rows x 4831 columns]\n",
      "removed 3568 Features\n"
     ]
    }
   ],
   "source": [
    "#Exploring the first five entries of the dataset to show the features that have been collected\n",
    "print(df.head())\n",
    "old_shape = df.shape\n",
    "#remove features that are not supported \n",
    "df = df.loc[:, (df.sum(axis=0) != 0)]\n",
    "new_shape = df.shape\n",
    "print(f'removed {old_shape[1] - new_shape[1]} Features')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old number of features: 1262\n",
      "             New number of features: 498\n",
      "             Number of features removed 764\n"
     ]
    }
   ],
   "source": [
    "X = df.drop('malware', axis=1)\n",
    "y = df['malware']\n",
    "## Binarize the output\n",
    "y = label_binarize(y, classes=['Benign', 'Malware'])\n",
    "y = y.ravel()\n",
    "\n",
    "\n",
    "variance = 0.1\n",
    "\n",
    "def getVarianceFeatures(X, y, var):\n",
    "    variancefs = VarianceThreshold(threshold=var).fit(X)\n",
    "    cols = variancefs.get_support(indices=True)\n",
    "    new_X = X.iloc[:,cols]\n",
    "    lostFeatures = X.shape[1] - new_X.shape[1]\n",
    "    \n",
    "    print(f'Old number of features: {X.shape[1]}\\n \\\n",
    "            New number of features: {new_X.shape[1]}\\n \\\n",
    "            Number of features removed {lostFeatures}')\n",
    "    \n",
    "    return new_X\n",
    "\n",
    "\n",
    "\n",
    "X = getVarianceFeatures(X, y, variance)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, \n",
    "                                                    random_state = 101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9242424242424242\n",
      "f1: 0.8535564853556485\n",
      "roc_auc: 0.8901958456973293\n",
      "recall: 0.816\n",
      "precision: 0.8947368421052632\n",
      "[[325  12]\n",
      " [ 23 102]]\n"
     ]
    }
   ],
   "source": [
    "baseMetrics = {}\n",
    "Optimisers = {'Base Metrics': baseMetrics}\n",
    "clf = SVC()\n",
    "clf.fit(X_train,y_train)\n",
    "pred = clf.predict(X_test)\n",
    "\n",
    "baseMetrics['accuracy'] = accuracy_score(y_test, pred)\n",
    "baseMetrics['f1'] = f1_score(y_test, pred)\n",
    "baseMetrics['roc_auc'] = roc_auc_score(y_test, pred)\n",
    "baseMetrics['recall'] = recall_score(y_test, pred)\n",
    "baseMetrics['precision'] = precision_score(y_test, pred)\n",
    "\n",
    "\n",
    "for metric, score in baseMetrics.items():\n",
    "    print(f'{metric}: {score}')\n",
    "    \n",
    "print(confusion_matrix(y_test,pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RandomSearch(clf, search_space, X_train, X_test, y_train, y_test):\n",
    "    randomSearchMetrics = {}\n",
    "    clf = clf\n",
    "\n",
    "    randomModel = RandomizedSearchCV(estimator = clf, param_distributions = search_space, n_iter = 80, \n",
    "                                   cv = 10, verbose= 5, random_state= 101, n_jobs = -1)\n",
    "\n",
    "\n",
    "\n",
    "    randomModel.fit(X_train,y_train)\n",
    "    pred = randomModel.predict(X_test)\n",
    "\n",
    "    print(randomModel.best_params_)\n",
    "\n",
    "\n",
    "    randomSearchMetrics['accuracy'] = accuracy_score(y_test, pred)\n",
    "    randomSearchMetrics['f1'] = f1_score(y_test, pred)\n",
    "    randomSearchMetrics['roc_auc'] = roc_auc_score(y_test, pred)\n",
    "    randomSearchMetrics['recall'] = recall_score(y_test, pred)\n",
    "    randomSearchMetrics['precision'] = precision_score(y_test, pred)\n",
    "\n",
    "\n",
    "    return  randomSearchMetrics, randomModel.best_params_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 80 candidates, totalling 800 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:    8.8s\n",
      "[Parallel(n_jobs=-1)]: Done 221 tasks      | elapsed:   23.7s\n",
      "[Parallel(n_jobs=-1)]: Done 408 tasks      | elapsed:   41.6s\n",
      "[Parallel(n_jobs=-1)]: Done 570 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 800 out of 800 | elapsed:  1.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tol': 0.001, 'shrinking': True, 'kernel': 'rbf', 'gamma': 'scale', 'degree': 6, 'coef0': 0.1, 'C': 10}\n",
      "accuracy: 0.9437229437229437\n",
      "f1: 0.8968253968253967\n",
      "roc_auc: 0.9312284866468843\n",
      "recall: 0.904\n",
      "precision: 0.889763779527559\n"
     ]
    }
   ],
   "source": [
    "search_space = {'C': [0.01, 0.1, 0.5, 1, 10],\n",
    "                'kernel': ['linear', 'poly', 'rbf','sigmoid'],\n",
    "                'degree': [1, 3, 6, 9],\n",
    "                'gamma': ['scale', 'auto'],\n",
    "                'coef0': [0.0, 0.01, 0.1 , 1.0, 10.0],\n",
    "                'shrinking': [True, False],\n",
    "                'tol': [0.001, 0.1, 1.0, 10.0]\n",
    "                }\n",
    "\n",
    "randomSearchMetrics, randomModel = RandomSearch(SVC(), search_space, X_train, X_test, y_train, y_test)\n",
    "Optimisers['Random Search'] = randomSearchMetrics\n",
    "for metric, score in randomSearchMetrics.items():\n",
    "    print(f'{metric}: {score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 81 candidates, totalling 810 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:    9.1s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:   20.6s\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed:   38.9s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 640 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 810 out of 810 | elapsed:  1.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 5.0, 'coef0': 0.05, 'degree': 3.0, 'gamma': 'scale', 'kernel': 'rbf', 'shrinking': True, 'tol': 0.0005}\n",
      "accuracy: 0.9415584415584416\n",
      "f1: 0.8932806324110673\n",
      "roc_auc: 0.9297448071216617\n",
      "recall: 0.904\n",
      "precision: 0.8828125\n",
      "[[322  15]\n",
      " [ 12 113]]\n"
     ]
    }
   ],
   "source": [
    "gridSearchMetrics = {}\n",
    "Optimisers['Grid Search'] = gridSearchMetrics\n",
    "\n",
    "#use features of random search to refine grid search\n",
    "#divisions rounded up to nearest whole int to avoid floats\n",
    "\n",
    "\n",
    "grid_search = {\n",
    "    'C': [randomModel['C'] / 2 , randomModel['C'], randomModel['C'] * 2],\n",
    "    'kernel': [randomModel['kernel']],\n",
    "    'degree': [round(randomModel['degree']) / 2 , randomModel['degree'], randomModel['degree'] * 2],\n",
    "    'gamma': [ randomModel['gamma']],\n",
    "    'coef0': [randomModel['coef0'] / 2 , randomModel['coef0'], randomModel['coef0'] * 2],\n",
    "    'shrinking': [randomModel['shrinking']],\n",
    "    'tol': [randomModel['tol'] / 2,  randomModel['tol'], randomModel['tol'] * 2]\n",
    "}\n",
    "\n",
    "Gridmodel = GridSearchCV(estimator = clf, param_grid = grid_search, \n",
    "                               cv = 10, verbose= 5, n_jobs = -1)\n",
    "Gridmodel.fit(X_train,y_train)\n",
    "\n",
    "pred = Gridmodel.best_estimator_.predict(X_test)\n",
    "baseMetrics['accuracy'] = np.array(cross_val_score(clf, X, y, cv=10, scoring='accuracy')).mean()\n",
    "baseMetrics['f1'] = np.array(cross_val_score(clf, X, y, cv=10, scoring='f1')).mean()\n",
    "baseMetrics['roc_auc'] = np.array(cross_val_score(clf, X, y, cv=10, scoring='roc_auc')).mean()\n",
    "baseMetrics['recall'] = np.array(cross_val_score(clf, X, y, cv=10, scoring='recall')).mean()\n",
    "baseMetrics['precision'] = np.array(cross_val_score(clf, X, y, cv=10, scoring='precision')).mean()\n",
    "\n",
    "print(Gridmodel.best_params_) \n",
    "\n",
    "for metric, score in gridSearchMetrics.items():\n",
    "    print(f'{metric}: {score}')\n",
    "    \n",
    "print(confusion_matrix(y_test,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [03:11<00:00,  1.04trial/s, best loss: -0.9544305988231223]\n",
      "{'C': 4.2743666261240945, 'coef0': 8.514047079700326, 'degree': 2.0, 'gamma': 0, 'kernel': 2, 'shrinking': 1, 'tol': 0.6901563047481236}\n"
     ]
    }
   ],
   "source": [
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
    "\n",
    "                    \n",
    "space = {'C': hp.uniform('C', 0.001, 10),\n",
    "        'kernel': hp.choice('kernel', ['linear', 'poly', 'rbf','sigmoid']),\n",
    "        'degree': hp.quniform('degree', 1,9,1),\n",
    "        'gamma': hp.choice ('gamma', ['scale', 'auto']),\n",
    "        'coef0' : hp.uniform ('coef0', 0.0, 10.0),\n",
    "        'shrinking' : hp.choice('shrinking', [True, False]),\n",
    "        'tol' : hp.uniform('tol', 0.001, 10.0)\n",
    "\n",
    "    }\n",
    "\n",
    "def objective(space):\n",
    "    model = SVC(C = space['C'], \n",
    "                          kernel = space['kernel'],\n",
    "                          degree = space['degree'],\n",
    "                          gamma = space['gamma'],\n",
    "                          coef0 = space['coef0'],\n",
    "                          shrinking = space['shrinking'],\n",
    "                          tol = space['tol']\n",
    "                         )\n",
    "    \n",
    "    accuracy = cross_val_score(model, X_train, y_train, cv = 10).mean()\n",
    "\n",
    "    # We aim to maximize accuracy, therefore we return it as a negative value\n",
    "    return {'loss': -accuracy, 'status': STATUS_OK }\n",
    "    \n",
    "trials = Trials()\n",
    "best = fmin(fn= objective,\n",
    "            space= space,\n",
    "            algo= tpe.suggest,\n",
    "            max_evals = 200,\n",
    "            trials= trials)\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9307359307359307\n",
      "f1: 0.8814814814814814\n",
      "roc_auc: 0.9374243323442136\n",
      "recall: 0.952\n",
      "precision: 0.8206896551724138\n",
      "[[311  26]\n",
      " [  6 119]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "bayesianMetrics = {}\n",
    "Optimisers['Bayesian'] = bayesianMetrics\n",
    "\n",
    "#convert the ints back into parameters\n",
    "loss_ = {0: 'hinge', 1: 'log', 2: 'modified_huber', 3: 'squared_hinge', 4: 'perceptron'}\n",
    "penalty_ = {0: 'l2', 1: 'l1', 2: 'elasticnet'}\n",
    "learning_rate_ = {0: 'invscaling', 1: 'constant'}\n",
    "fit_intercept_ = {0: True, 1: False}\n",
    "\n",
    "\n",
    "trainedforest = SVC(loss = loss_[best['loss']], \n",
    "                              penalty = penalty_[best['penalty']], \n",
    "                              alpha = best['alpha'], \n",
    "                              learning_rate = learning_rate_[best['learning_rate']], \n",
    "                              fit_intercept = fit_intercept_[best['fit_intercept']], \n",
    "                              l1_ratio = best['l1_ratio'],\n",
    "                              eta0 = best['eta0'],\n",
    "                              power_t = best['power_t']\n",
    "                            ).fit(X_train,y_train)\n",
    "pred = trainedforest.predict(X_test)\n",
    "\n",
    "bayesianMetrics['accuracy'] = accuracy_score(y_test, pred)\n",
    "bayesianMetrics['f1'] = f1_score(y_test, pred)\n",
    "bayesianMetrics['roc_auc'] = roc_auc_score(y_test, pred)\n",
    "bayesianMetrics['recall'] = recall_score(y_test, pred)\n",
    "bayesianMetrics['precision'] = precision_score(y_test, pred)\n",
    "\n",
    "for metric, score in bayesianMetrics.items():\n",
    "    print(f'{metric}: {score}')\n",
    "    \n",
    "print(confusion_matrix(y_test,pred))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hpsklearn import HyperoptEstimator, svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 11.49trial/s, best loss: 0.06511627906976747]\n",
      "100%|██████████| 2/2 [00:00<00:00, 28.35trial/s, best loss: 0.046511627906976716]\n",
      "100%|██████████| 3/3 [00:00<00:00, 49.19trial/s, best loss: 0.046511627906976716]\n",
      "100%|██████████| 4/4 [00:00<00:00, 59.73trial/s, best loss: 0.046511627906976716]\n",
      "100%|██████████| 5/5 [00:00<00:00, 78.05trial/s, best loss: 0.046511627906976716]\n",
      "100%|██████████| 6/6 [00:00<00:00, 86.17trial/s, best loss: 0.046511627906976716]\n",
      "100%|██████████| 7/7 [00:00<00:00, 114.79trial/s, best loss: 0.046511627906976716]\n",
      "100%|██████████| 8/8 [00:00<00:00, 118.37trial/s, best loss: 0.046511627906976716]\n",
      "100%|██████████| 9/9 [00:00<00:00, 110.63trial/s, best loss: 0.046511627906976716]\n",
      "100%|██████████| 10/10 [00:00<00:00, 144.34trial/s, best loss: 0.046511627906976716]\n",
      "100%|██████████| 11/11 [00:00<00:00, 113.32trial/s, best loss: 0.046511627906976716]\n",
      "100%|██████████| 12/12 [00:00<00:00, 190.24trial/s, best loss: 0.046511627906976716]\n",
      "100%|██████████| 13/13 [00:00<00:00, 232.54trial/s, best loss: 0.046511627906976716]\n",
      "100%|██████████| 14/14 [00:00<00:00, 218.35trial/s, best loss: 0.046511627906976716]\n",
      "100%|██████████| 15/15 [00:00<00:00, 56.04trial/s, best loss: 0.046511627906976716]\n",
      "100%|██████████| 16/16 [00:00<00:00, 130.30trial/s, best loss: 0.046511627906976716]\n",
      "100%|██████████| 17/17 [00:00<00:00, 210.65trial/s, best loss: 0.046511627906976716]\n",
      "100%|██████████| 18/18 [00:00<00:00, 137.60trial/s, best loss: 0.046511627906976716]\n",
      "100%|██████████| 19/19 [00:00<00:00, 305.31trial/s, best loss: 0.046511627906976716]\n",
      "100%|██████████| 20/20 [00:00<00:00, 258.62trial/s, best loss: 0.046511627906976716]\n",
      "100%|██████████| 21/21 [00:00<00:00, 32.04trial/s, best loss: 0.046511627906976716]\n",
      "100%|██████████| 22/22 [00:00<00:00, 198.06trial/s, best loss: 0.046511627906976716]\n",
      "100%|██████████| 23/23 [00:00<00:00, 194.99trial/s, best loss: 0.046511627906976716]\n",
      "100%|██████████| 24/24 [00:00<00:00, 45.29trial/s, best loss: 0.046511627906976716]\n",
      "100%|██████████| 25/25 [00:00<00:00, 234.86trial/s, best loss: 0.046511627906976716]\n",
      "100%|██████████| 26/26 [00:00<00:00, 182.06trial/s, best loss: 0.046511627906976716]\n",
      "100%|██████████| 27/27 [00:01<00:00, 26.25trial/s, best loss: 0.046511627906976716]\n",
      "100%|██████████| 28/28 [00:00<00:00, 74.74trial/s, best loss: 0.046511627906976716]\n",
      "100%|██████████| 29/29 [00:00<00:00, 295.67trial/s, best loss: 0.046511627906976716]\n",
      "100%|██████████| 30/30 [00:00<00:00, 259.22trial/s, best loss: 0.046511627906976716]\n",
      "100%|██████████| 31/31 [00:00<00:00, 417.85trial/s, best loss: 0.046511627906976716]\n",
      "100%|██████████| 32/32 [00:00<00:00, 218.78trial/s, best loss: 0.046511627906976716]\n",
      "100%|██████████| 33/33 [00:00<00:00, 212.49trial/s, best loss: 0.046511627906976716]\n",
      "100%|██████████| 34/34 [00:00<00:00, 445.45trial/s, best loss: 0.046511627906976716]\n",
      "100%|██████████| 35/35 [00:00<00:00, 504.96trial/s, best loss: 0.046511627906976716]\n",
      "100%|██████████| 36/36 [00:00<00:00, 452.97trial/s, best loss: 0.046511627906976716]\n",
      "100%|██████████| 37/37 [00:00<00:00, 490.33trial/s, best loss: 0.046511627906976716]\n",
      "100%|██████████| 38/38 [00:00<00:00, 527.59trial/s, best loss: 0.046511627906976716]\n",
      "100%|██████████| 39/39 [00:00<00:00, 463.70trial/s, best loss: 0.046511627906976716]\n",
      "100%|██████████| 40/40 [00:00<00:00, 453.41trial/s, best loss: 0.046511627906976716]\n",
      "100%|██████████| 41/41 [00:00<00:00, 436.45trial/s, best loss: 0.046511627906976716]\n",
      "100%|██████████| 42/42 [00:00<00:00, 101.07trial/s, best loss: 0.046511627906976716]\n",
      "100%|██████████| 43/43 [00:00<00:00, 626.03trial/s, best loss: 0.046511627906976716]\n",
      "100%|██████████| 44/44 [00:00<00:00, 402.95trial/s, best loss: 0.046511627906976716]\n",
      "100%|██████████| 45/45 [00:00<00:00, 283.03trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 46/46 [00:00<00:00, 459.51trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 47/47 [00:00<00:00, 405.41trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 48/48 [00:00<00:00, 260.47trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 49/49 [00:00<00:00, 449.33trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 50/50 [00:00<00:00, 380.34trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 51/51 [00:00<00:00, 446.83trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 52/52 [00:00<00:00, 550.17trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 53/53 [00:00<00:00, 616.08trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 54/54 [00:00<00:00, 242.86trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 55/55 [00:00<00:00, 555.98trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 56/56 [00:00<00:00, 667.86trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 57/57 [00:00<00:00, 619.78trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 58/58 [00:00<00:00, 513.45trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 59/59 [00:00<00:00, 131.14trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 60/60 [00:00<00:00, 589.08trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 61/61 [00:00<00:00, 545.65trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 62/62 [00:00<00:00, 698.17trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 63/63 [00:00<00:00, 539.34trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 64/64 [00:00<00:00, 584.38trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 65/65 [00:00<00:00, 333.27trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 66/66 [00:00<00:00, 524.42trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 67/67 [00:00<00:00, 679.78trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 68/68 [00:00<00:00, 809.37trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 69/69 [00:00<00:00, 825.94trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 70/70 [00:00<00:00, 685.94trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 71/71 [00:00<00:00, 558.08trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 72/72 [00:00<00:00, 677.73trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 73/73 [00:00<00:00, 663.82trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 74/74 [00:00<00:00, 990.86trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 75/75 [00:00<00:00, 881.01trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 76/76 [00:00<00:00, 668.36trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 77/77 [00:00<00:00, 862.51trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 78/78 [00:00<00:00, 1000.02trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 79/79 [00:00<00:00, 944.35trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 80/80 [00:00<00:00, 823.17trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 81/81 [00:00<00:00, 204.91trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 82/82 [00:00<00:00, 240.51trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 83/83 [00:00<00:00, 704.13trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 84/84 [00:00<00:00, 646.25trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 85/85 [00:00<00:00, 1070.42trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 86/86 [00:00<00:00, 673.95trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 87/87 [00:00<00:00, 719.03trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 88/88 [00:00<00:00, 259.33trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 89/89 [00:00<00:00, 648.40trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 90/90 [00:00<00:00, 856.12trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 91/91 [00:00<00:00, 736.31trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 92/92 [00:00<00:00, 747.86trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 93/93 [00:00<00:00, 791.23trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 94/94 [00:00<00:00, 445.24trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 95/95 [00:00<00:00, 705.56trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 96/96 [00:00<00:00, 760.46trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 97/97 [00:00<00:00, 863.54trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 98/98 [00:00<00:00, 379.54trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 99/99 [00:00<00:00, 498.95trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 100/100 [00:00<00:00, 709.84trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 101/101 [00:00<00:00, 654.33trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 102/102 [00:00<00:00, 355.02trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 103/103 [00:00<00:00, 866.87trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 104/104 [00:00<00:00, 622.07trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 105/105 [00:00<00:00, 913.97trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 106/106 [00:00<00:00, 206.50trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 107/107 [00:00<00:00, 1127.01trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 108/108 [00:00<00:00, 872.68trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 109/109 [00:00<00:00, 184.23trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 110/110 [00:00<00:00, 113.12trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 111/111 [00:00<00:00, 895.37trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 112/112 [00:00<00:00, 747.16trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 113/113 [00:00<00:00, 1014.19trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 114/114 [00:00<00:00, 837.55trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 115/115 [00:00<00:00, 1206.32trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 116/116 [00:00<00:00, 425.60trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 117/117 [00:00<00:00, 828.21trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 118/118 [00:00<00:00, 984.56trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 119/119 [00:00<00:00, 292.99trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 120/120 [00:00<00:00, 606.98trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 121/121 [00:00<00:00, 952.79trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 122/122 [00:00<00:00, 769.11trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 123/123 [00:00<00:00, 1337.80trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 124/124 [00:00<00:00, 1138.16trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 125/125 [00:00<00:00, 957.39trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 126/126 [00:00<00:00, 1097.06trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 127/127 [00:00<00:00, 1443.96trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 128/128 [00:00<00:00, 1066.74trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 129/129 [00:00<00:00, 909.11trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 130/130 [00:00<00:00, 940.95trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 131/131 [00:00<00:00, 351.02trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 132/132 [00:00<00:00, 939.68trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 133/133 [00:00<00:00, 1168.63trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 134/134 [00:00<00:00, 1260.88trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 135/135 [00:00<00:00, 1015.75trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 136/136 [00:00<00:00, 1102.47trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 137/137 [00:00<00:00, 1698.07trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 138/138 [00:00<00:00, 1009.66trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 139/139 [00:00<00:00, 1175.50trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 140/140 [00:00<00:00, 254.49trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 141/141 [00:00<00:00, 664.84trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 142/142 [00:00<00:00, 729.42trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 143/143 [00:00<00:00, 470.61trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 144/144 [00:00<00:00, 968.18trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 145/145 [00:00<00:00, 1137.50trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 146/146 [00:00<00:00, 1256.46trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 147/147 [00:00<00:00, 1134.98trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 148/148 [00:00<00:00, 1167.30trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 149/149 [00:00<00:00, 1098.50trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 150/150 [00:00<00:00, 193.67trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 151/151 [00:00<00:00, 473.79trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 152/152 [00:00<00:00, 888.87trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 153/153 [00:00<00:00, 1076.87trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 154/154 [00:00<00:00, 1186.35trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 155/155 [00:00<00:00, 1237.05trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 156/156 [00:00<00:00, 1205.26trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 157/157 [00:00<00:00, 1428.08trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 158/158 [00:00<00:00, 1286.67trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 159/159 [00:00<00:00, 1231.30trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 160/160 [00:00<00:00, 1118.54trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 161/161 [00:00<00:00, 1693.95trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 162/162 [00:00<00:00, 1571.42trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 163/163 [00:00<00:00, 1489.85trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 164/164 [00:00<00:00, 1168.15trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 165/165 [00:00<00:00, 651.83trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 166/166 [00:00<00:00, 1263.69trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 167/167 [00:00<00:00, 1181.08trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 168/168 [00:00<00:00, 1757.71trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 169/169 [00:00<00:00, 1935.19trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 170/170 [00:00<00:00, 1294.24trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 171/171 [00:02<00:00, 57.64trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 172/172 [00:00<00:00, 861.61trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 173/173 [00:00<00:00, 1380.04trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 174/174 [00:00<00:00, 1437.69trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 175/175 [00:00<00:00, 1291.65trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 176/176 [00:00<00:00, 1276.07trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 177/177 [00:00<00:00, 1350.44trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 178/178 [00:00<00:00, 1863.02trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 179/179 [00:00<00:00, 1308.58trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 180/180 [00:00<00:00, 1430.74trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 181/181 [00:00<00:00, 700.76trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 182/182 [00:00<00:00, 483.00trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 183/183 [00:00<00:00, 1800.27trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 184/184 [00:00<00:00, 650.82trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 185/185 [00:00<00:00, 1404.64trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 186/186 [00:00<00:00, 594.36trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 187/187 [00:00<00:00, 929.50trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 188/188 [00:00<00:00, 1534.99trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 189/189 [00:00<00:00, 1775.85trial/s, best loss: 0.041860465116279055]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 190/190 [00:00<00:00, 2017.94trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 191/191 [00:00<00:00, 1498.82trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 192/192 [00:00<00:00, 1335.61trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 193/193 [00:00<00:00, 1389.48trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 194/194 [00:00<00:00, 1439.03trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 195/195 [00:00<00:00, 1821.72trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 196/196 [00:00<00:00, 869.19trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 197/197 [00:00<00:00, 1424.55trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 198/198 [00:00<00:00, 391.74trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 199/199 [00:00<00:00, 1187.83trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 200/200 [00:00<00:00, 316.61trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 201/201 [00:00<00:00, 1678.52trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 202/202 [00:00<00:00, 1564.26trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 203/203 [00:00<00:00, 1493.68trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 204/204 [00:00<00:00, 1477.24trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 205/205 [00:00<00:00, 1429.08trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 206/206 [00:00<00:00, 1514.36trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 207/207 [00:00<00:00, 438.88trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 208/208 [00:00<00:00, 2124.17trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 209/209 [00:00<00:00, 1533.96trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 210/210 [00:00<00:00, 1986.17trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 211/211 [00:00<00:00, 1399.38trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 212/212 [00:00<00:00, 1336.97trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 213/213 [00:00<00:00, 2374.07trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 214/214 [00:00<00:00, 1434.88trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 215/215 [00:00<00:00, 914.66trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1578.85trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 217/217 [00:00<00:00, 1714.29trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 218/218 [00:00<00:00, 1754.09trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 219/219 [00:00<00:00, 2346.59trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 220/220 [00:00<00:00, 2312.56trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 221/221 [00:00<00:00, 1957.05trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 222/222 [00:00<00:00, 1999.09trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 223/223 [00:00<00:00, 1706.62trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 224/224 [00:00<00:00, 973.88trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 225/225 [00:00<00:00, 1707.54trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 226/226 [00:00<00:00, 2177.19trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 227/227 [00:00<00:00, 1675.96trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 228/228 [00:00<00:00, 799.85trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 229/229 [00:00<00:00, 671.04trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 230/230 [00:00<00:00, 1623.90trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 231/231 [00:00<00:00, 2335.56trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 232/232 [00:00<00:00, 1003.94trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 233/233 [00:00<00:00, 2019.99trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 234/234 [00:00<00:00, 1593.53trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 235/235 [00:00<00:00, 2019.36trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 236/236 [00:00<00:00, 1719.94trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 237/237 [00:00<00:00, 1769.86trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 238/238 [00:00<00:00, 2086.78trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 239/239 [00:00<00:00, 1507.17trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 240/240 [00:00<00:00, 1705.66trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 241/241 [00:00<00:00, 1768.72trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 242/242 [00:00<00:00, 1947.00trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 243/243 [00:00<00:00, 2638.80trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 244/244 [00:00<00:00, 1456.03trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 245/245 [00:00<00:00, 1799.85trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 246/246 [00:00<00:00, 2045.81trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 247/247 [00:00<00:00, 1628.63trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 248/248 [00:00<00:00, 1322.13trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 249/249 [00:00<00:00, 1402.76trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 250/250 [00:00<00:00, 477.58trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 251/251 [00:00<00:00, 1889.57trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 252/252 [00:00<00:00, 1760.59trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 253/253 [00:00<00:00, 1737.55trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 254/254 [00:00<00:00, 1426.69trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 255/255 [00:00<00:00, 2036.52trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 256/256 [00:00<00:00, 340.15trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 257/257 [00:00<00:00, 1991.74trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 258/258 [00:00<00:00, 1480.31trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 259/259 [00:00<00:00, 1927.98trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 260/260 [00:00<00:00, 1982.84trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 261/261 [00:00<00:00, 1721.87trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 262/262 [00:00<00:00, 1953.94trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 263/263 [00:00<00:00, 527.54trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 264/264 [00:00<00:00, 1493.84trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 265/265 [00:00<00:00, 1930.46trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 266/266 [00:00<00:00, 2253.43trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 267/267 [00:00<00:00, 2269.15trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 268/268 [00:00<00:00, 1947.88trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 269/269 [00:00<00:00, 1945.82trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 270/270 [00:00<00:00, 1919.22trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 271/271 [00:00<00:00, 551.12trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 272/272 [00:00<00:00, 1954.73trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 273/273 [00:00<00:00, 1793.39trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 274/274 [00:00<00:00, 1830.31trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 275/275 [00:00<00:00, 2045.99trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 276/276 [00:00<00:00, 1781.57trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 277/277 [00:00<00:00, 1630.14trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 278/278 [00:00<00:00, 1844.90trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 279/279 [00:00<00:00, 1505.09trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 280/280 [00:00<00:00, 924.31trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 281/281 [00:00<00:00, 1666.33trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 282/282 [00:00<00:00, 2098.27trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 283/283 [00:00<00:00, 1728.59trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 284/284 [00:01<00:00, 241.01trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 285/285 [00:00<00:00, 1771.42trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 286/286 [00:00<00:00, 1915.41trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 287/287 [00:00<00:00, 417.66trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 288/288 [00:00<00:00, 2039.88trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 289/289 [00:00<00:00, 2640.21trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 290/290 [00:00<00:00, 2058.35trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 291/291 [00:00<00:00, 2311.31trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 292/292 [00:00<00:00, 2099.28trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 293/293 [00:00<00:00, 729.76trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 294/294 [00:00<00:00, 2176.04trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 295/295 [00:00<00:00, 2327.81trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 296/296 [00:00<00:00, 2147.98trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 297/297 [00:00<00:00, 1042.80trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 298/298 [00:00<00:00, 1034.40trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 299/299 [00:00<00:00, 2032.85trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 300/300 [00:00<00:00, 2675.64trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 301/301 [00:00<00:00, 1917.06trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 302/302 [00:00<00:00, 1898.78trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 303/303 [00:00<00:00, 2183.83trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 304/304 [00:00<00:00, 2112.81trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 305/305 [00:00<00:00, 355.66trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 306/306 [00:00<00:00, 2202.33trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 307/307 [00:00<00:00, 2073.66trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 308/308 [00:00<00:00, 720.20trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 309/309 [00:00<00:00, 2025.05trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 310/310 [00:00<00:00, 2604.66trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 311/311 [00:00<00:00, 1953.01trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 312/312 [00:00<00:00, 1081.43trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 313/313 [00:00<00:00, 753.90trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 314/314 [00:00<00:00, 3185.67trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 315/315 [00:00<00:00, 2121.60trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 316/316 [00:00<00:00, 2249.17trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 317/317 [00:01<00:00, 262.45trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 318/318 [00:00<00:00, 1153.20trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 319/319 [00:00<00:00, 1569.12trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 320/320 [00:00<00:00, 2376.84trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 321/321 [00:00<00:00, 2173.68trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 322/322 [00:00<00:00, 1803.03trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 323/323 [00:00<00:00, 2179.64trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 324/324 [00:00<00:00, 589.74trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 325/325 [00:00<00:00, 936.24trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 326/326 [00:00<00:00, 2296.28trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 327/327 [00:00<00:00, 1614.46trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 328/328 [00:00<00:00, 2487.52trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 329/329 [00:00<00:00, 2850.24trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 330/330 [00:00<00:00, 2458.69trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 331/331 [00:00<00:00, 1963.55trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 332/332 [00:00<00:00, 2731.52trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 333/333 [00:00<00:00, 2174.64trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 334/334 [00:00<00:00, 2561.11trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 335/335 [00:00<00:00, 2422.84trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 336/336 [00:00<00:00, 2287.91trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 337/337 [00:00<00:00, 2697.04trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 338/338 [00:00<00:00, 2698.29trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 339/339 [00:00<00:00, 2157.09trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 340/340 [00:00<00:00, 2762.45trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 341/341 [00:00<00:00, 3023.82trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 342/342 [00:00<00:00, 1074.40trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 343/343 [00:00<00:00, 540.09trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 344/344 [00:00<00:00, 1972.62trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 345/345 [00:00<00:00, 2240.43trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 346/346 [00:00<00:00, 2345.55trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 347/347 [00:00<00:00, 2324.78trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 348/348 [00:00<00:00, 3068.09trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 349/349 [00:00<00:00, 3327.43trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 350/350 [00:00<00:00, 2909.15trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 351/351 [00:00<00:00, 1519.33trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 352/352 [00:00<00:00, 1484.81trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 353/353 [00:00<00:00, 2828.03trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 354/354 [00:00<00:00, 2968.26trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 355/355 [00:00<00:00, 2207.17trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 356/356 [00:00<00:00, 2606.03trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 357/357 [00:00<00:00, 2726.72trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 358/358 [00:00<00:00, 2642.39trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 359/359 [00:00<00:00, 3351.54trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 360/360 [00:00<00:00, 438.34trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 361/361 [00:00<00:00, 2537.60trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 362/362 [00:00<00:00, 1456.78trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 363/363 [00:00<00:00, 2654.05trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 364/364 [00:00<00:00, 2233.46trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 365/365 [00:00<00:00, 2459.86trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 366/366 [00:00<00:00, 2642.65trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 367/367 [00:00<00:00, 2331.06trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 368/368 [00:00<00:00, 612.16trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 369/369 [00:00<00:00, 2472.92trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 370/370 [00:00<00:00, 3061.99trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 371/371 [00:00<00:00, 2561.12trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 372/372 [00:00<00:00, 2695.08trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 373/373 [00:00<00:00, 3247.94trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 374/374 [00:00<00:00, 3545.22trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 375/375 [00:00<00:00, 3131.47trial/s, best loss: 0.041860465116279055]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 376/376 [00:00<00:00, 3503.13trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 377/377 [00:00<00:00, 3335.05trial/s, best loss: 0.041860465116279055]\n",
      "100%|██████████| 378/378 [00:00<00:00, 2411.78trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 379/379 [00:00<00:00, 3086.75trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 380/380 [00:00<00:00, 3075.39trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 381/381 [00:00<00:00, 2775.76trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 382/382 [00:00<00:00, 2663.11trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 383/383 [00:00<00:00, 2200.76trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 384/384 [00:00<00:00, 3262.46trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 385/385 [00:00<00:00, 2712.18trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 386/386 [00:00<00:00, 2601.24trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 387/387 [00:00<00:00, 2291.53trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 388/388 [00:00<00:00, 2544.18trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 389/389 [00:00<00:00, 3208.21trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 390/390 [00:00<00:00, 3323.31trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 391/391 [00:00<00:00, 3056.21trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 392/392 [00:00<00:00, 2746.17trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 393/393 [00:00<00:00, 3073.33trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 394/394 [00:00<00:00, 2391.42trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 395/395 [00:00<00:00, 3333.05trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 396/396 [00:00<00:00, 2439.59trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 397/397 [00:00<00:00, 2599.29trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 398/398 [00:00<00:00, 1093.60trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 399/399 [00:00<00:00, 3309.90trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 400/400 [00:00<00:00, 2580.31trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 401/401 [00:00<00:00, 2819.73trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 402/402 [00:00<00:00, 2554.70trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 403/403 [00:00<00:00, 2353.38trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 404/404 [00:00<00:00, 2871.26trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 405/405 [00:00<00:00, 2476.61trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 406/406 [00:00<00:00, 1254.93trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 407/407 [00:00<00:00, 2728.78trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 408/408 [00:00<00:00, 2604.72trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 409/409 [00:00<00:00, 1217.73trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 410/410 [00:00<00:00, 1493.08trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 411/411 [00:00<00:00, 2830.42trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 412/412 [00:00<00:00, 2571.23trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 413/413 [00:00<00:00, 2297.35trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 414/414 [00:00<00:00, 2560.20trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 415/415 [00:00<00:00, 1443.96trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 416/416 [00:00<00:00, 2237.52trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 417/417 [00:00<00:00, 2463.25trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 418/418 [00:00<00:00, 2823.81trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 419/419 [00:00<00:00, 1319.61trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 420/420 [00:00<00:00, 1775.02trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 421/421 [00:00<00:00, 2595.66trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 422/422 [00:00<00:00, 1727.85trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 423/423 [00:00<00:00, 1807.94trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 424/424 [00:00<00:00, 2559.08trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 425/425 [00:00<00:00, 3531.65trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 426/426 [00:00<00:00, 2575.09trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 427/427 [00:00<00:00, 1234.80trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 428/428 [00:00<00:00, 1770.33trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 429/429 [00:00<00:00, 2666.21trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 430/430 [00:00<00:00, 3195.15trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 431/431 [00:00<00:00, 2863.85trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 432/432 [00:00<00:00, 2056.44trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 433/433 [00:00<00:00, 2863.37trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 434/434 [00:00<00:00, 2476.01trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 435/435 [00:00<00:00, 1410.74trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 436/436 [00:00<00:00, 3192.46trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 437/437 [00:00<00:00, 2914.01trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 438/438 [00:00<00:00, 1957.44trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 439/439 [00:00<00:00, 3145.12trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 440/440 [00:00<00:00, 2501.99trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 441/441 [00:00<00:00, 2873.83trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 442/442 [00:00<00:00, 1176.25trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 443/443 [00:00<00:00, 2679.42trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 444/444 [00:00<00:00, 2451.20trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 445/445 [00:00<00:00, 2554.76trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 446/446 [00:00<00:00, 2850.49trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 447/447 [00:00<00:00, 2062.30trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 448/448 [00:00<00:00, 3137.11trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 449/449 [00:00<00:00, 1998.29trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 450/450 [00:00<00:00, 3048.39trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 451/451 [00:00<00:00, 2825.79trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 452/452 [00:00<00:00, 2669.60trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 453/453 [00:00<00:00, 2699.83trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 454/454 [00:00<00:00, 2132.12trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 455/455 [00:00<00:00, 2237.88trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 456/456 [00:00<00:00, 3597.73trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 457/457 [00:00<00:00, 3457.79trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 458/458 [00:00<00:00, 2530.89trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 459/459 [00:00<00:00, 3465.09trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 460/460 [00:00<00:00, 950.25trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 461/461 [00:00<00:00, 2865.92trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 462/462 [00:00<00:00, 1413.56trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 463/463 [00:00<00:00, 3216.03trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 464/464 [00:00<00:00, 2086.93trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 465/465 [00:00<00:00, 3077.42trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 466/466 [00:00<00:00, 2313.38trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 467/467 [00:00<00:00, 3625.52trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 468/468 [00:00<00:00, 2807.56trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 469/469 [00:00<00:00, 3254.62trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 470/470 [00:00<00:00, 2293.07trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 471/471 [00:00<00:00, 2776.46trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 472/472 [00:00<00:00, 3149.75trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 473/473 [00:00<00:00, 2940.35trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 474/474 [00:00<00:00, 3492.77trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 475/475 [00:00<00:00, 2861.09trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 476/476 [00:00<00:00, 2736.12trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 477/477 [00:00<00:00, 2840.64trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 478/478 [00:00<00:00, 2972.47trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 479/479 [00:00<00:00, 3624.04trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 480/480 [00:00<00:00, 1753.62trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 481/481 [00:00<00:00, 3115.87trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 482/482 [00:00<00:00, 868.25trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 483/483 [00:00<00:00, 3834.24trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 484/484 [00:00<00:00, 3337.65trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 485/485 [00:00<00:00, 2951.67trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 486/486 [00:00<00:00, 3618.74trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 487/487 [00:00<00:00, 1217.78trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 488/488 [00:00<00:00, 2857.08trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 489/489 [00:00<00:00, 2911.85trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 490/490 [00:00<00:00, 989.42trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 491/491 [00:00<00:00, 2558.43trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 492/492 [00:00<00:00, 712.12trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 493/493 [00:00<00:00, 2909.84trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 494/494 [00:00<00:00, 2653.43trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 495/495 [00:00<00:00, 3516.46trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 496/496 [00:00<00:00, 1983.05trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 497/497 [00:00<00:00, 3129.13trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 498/498 [00:00<00:00, 2478.17trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 499/499 [00:00<00:00, 2145.31trial/s, best loss: 0.037209302325581395]\n",
      "100%|██████████| 500/500 [00:00<00:00, 2681.13trial/s, best loss: 0.037209302325581395]\n",
      "0.9264069264069265\n",
      "{'learner': SGDClassifier(alpha=3.5551124276058275e-05, average=False,\n",
      "              class_weight='balanced', early_stopping=False, epsilon=0.1,\n",
      "              eta0=0.057615032359113263, fit_intercept=True,\n",
      "              l1_ratio=0.5016334128226829, learning_rate='constant',\n",
      "              loss='modified_huber', max_iter=102539159.0, n_iter_no_change=5,\n",
      "              n_jobs=1, penalty='l1', power_t=0.6259697404380353,\n",
      "              random_state=4, shuffle=True, tol=0.0012888409860024497,\n",
      "              validation_fraction=0.1, verbose=False, warm_start=False), 'preprocs': (), 'ex_preprocs': ()}\n",
      "accuracy: 0.9264069264069265\n",
      "f1: 0.8768115942028986\n",
      "roc_auc: 0.9394896142433236\n",
      "recall: 0.968\n",
      "precision: 0.8013245033112583\n",
      "[[307  30]\n",
      " [  4 121]]\n"
     ]
    }
   ],
   "source": [
    "AutobayesianMetrics = {}\n",
    "Optimisers['Auto Bayesian'] = AutobayesianMetrics\n",
    "\n",
    "estim = HyperoptEstimator(classifier=svc('svm'),\n",
    "                          preprocessing=[],\n",
    "                          algo=tpe.suggest,\n",
    "                          max_evals=500,\n",
    "                          trial_timeout=120)\n",
    "\n",
    "# Search the hyperparameter space based on the data\n",
    "estim.fit(X_train.values, y_train)\n",
    "\n",
    "# Show the results\n",
    "\n",
    "print(estim.score(X_test.values, y_test))\n",
    "# 1.0\n",
    "\n",
    "print( estim.best_model() )\n",
    "\n",
    "model = estim.best_model()\n",
    "trainedsvm = model['learner']\n",
    "pred = trainedsvm.predict(X_test)\n",
    "\n",
    "AutobayesianMetrics['accuracy'] = accuracy_score(y_test, pred)\n",
    "AutobayesianMetrics['f1'] = f1_score(y_test, pred)\n",
    "AutobayesianMetrics['roc_auc'] = roc_auc_score(y_test, pred)\n",
    "AutobayesianMetrics['recall'] = recall_score(y_test, pred)\n",
    "AutobayesianMetrics['precision'] = precision_score(y_test, pred)\n",
    "\n",
    "for metric, score in AutobayesianMetrics.items():\n",
    "    print(f'{metric}: {score}')\n",
    "    \n",
    "print(confusion_matrix(y_test,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[311  26]\n",
      " [  6 119]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.92      0.95       337\n",
      "           1       0.82      0.95      0.88       125\n",
      "\n",
      "    accuracy                           0.93       462\n",
      "   macro avg       0.90      0.94      0.92       462\n",
      "weighted avg       0.94      0.93      0.93       462\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = estim.best_model()\n",
    "trainedsvm = model['learner']\n",
    "predictionforest = trainedforest.predict(X_test)\n",
    "print(confusion_matrix(y_test,predictionforest))\n",
    "print(classification_report(y_test,predictionforest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6f41aed7bef4e9eafba7f5795cdb9cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Optimization Progress', max=10100.0, style=ProgressStyle(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "There was an error in the TPOT optimization process. This could be because the data was not formatted properly, or because data for a regression problem was provided to the TPOTClassifier object. Please make sure you passed the data to TPOT correctly.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tpot/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, features, target, sample_weight, groups)\u001b[0m\n\u001b[1;32m    710\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbosity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 711\u001b[0;31m                     \u001b[0mper_generation_function\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_periodic_pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    712\u001b[0m                 )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tpot/gp_deap.py\u001b[0m in \u001b[0;36meaMuPlusLambda\u001b[0;34m(population, toolbox, mu, lambda_, cxpb, mutpb, ngen, pbar, stats, halloffame, verbose, per_generation_function)\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mper_generation_function\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m             \u001b[0mper_generation_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m         \u001b[0;31m# Vary the population\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tpot/base.py\u001b[0m in \u001b[0;36m_check_periodic_pipeline\u001b[0;34m(self, gen)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         \"\"\"\n\u001b[0;32m-> 1003\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_top_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1004\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperiodic_checkpoint_folder\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tpot/base.py\u001b[0m in \u001b[0;36m_update_top_pipeline\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    792\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimized_pipeline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 793\u001b[0;31m                 raise RuntimeError('There was an error in the TPOT optimization '\n\u001b[0m\u001b[1;32m    794\u001b[0m                                    \u001b[0;34m'process. This could be because the data was '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: There was an error in the TPOT optimization process. This could be because the data was not formatted properly, or because data for a regression problem was provided to the TPOTClassifier object. Please make sure you passed the data to TPOT correctly.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-f1bbec2cee99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m tpot = TPOTClassifier(generations= 100, population_size= 100,\n\u001b[1;32m     21\u001b[0m                                  verbosity= 2, n_jobs=-1, config_dict=tpot_config, early_stop=50)\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mtpot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtpot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtpot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tpot/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, features, target, sample_weight, groups)\u001b[0m\n\u001b[1;32m    740\u001b[0m                     \u001b[0;31m# raise the exception if it's our last attempt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mattempt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mattempts\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 742\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    743\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tpot/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, features, target, sample_weight, groups)\u001b[0m\n\u001b[1;32m    731\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_top_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    734\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_summary_of_best_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m                     \u001b[0;31m# Delete the temporary cache before exiting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tpot/base.py\u001b[0m in \u001b[0;36m_update_top_pipeline\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    791\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimized_pipeline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 793\u001b[0;31m                 raise RuntimeError('There was an error in the TPOT optimization '\n\u001b[0m\u001b[1;32m    794\u001b[0m                                    \u001b[0;34m'process. This could be because the data was '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    795\u001b[0m                                    \u001b[0;34m'not formatted properly, or because data for '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: There was an error in the TPOT optimization process. This could be because the data was not formatted properly, or because data for a regression problem was provided to the TPOTClassifier object. Please make sure you passed the data to TPOT correctly."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# from tpot import TPOTClassifier\n",
    "\n",
    "\n",
    "# GeneticMetrics = {}\n",
    "# Optimisers['Genetic'] = GeneticMetrics\n",
    "\n",
    "# tpot_config = {\n",
    "#     'sklearn.svm.SVC': {\n",
    "#         'loss': ['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron'],\n",
    "#         'penalty': ['l2', 'l1', 'elasticnet'],\n",
    "#         'alpha': [0.0001, 0.001, 0.01],\n",
    "#         'learning_rate': ['invscaling', 'constant', 'optimal'],\n",
    "#         'fit_intercept': [True, False],\n",
    "#         'l1_ratio': [0.15, 0.25, 0.0, 1.0, 0.75, 0.5],\n",
    "#         'eta0': [0.01, 0.1, 1.0],\n",
    "#         'power_t': [0.5, 0.0, 1.0, 0.1, 100.0, 10.0, 50.0]\n",
    "#     }\n",
    "# }\n",
    "        \n",
    "# tpot = TPOTClassifier(generations= 100, population_size= 100,\n",
    "#                                  verbosity= 2, n_jobs=-1, config_dict=tpot_config, early_stop=50)\n",
    "# tpot.fit(X_train, y_train)\n",
    "# pred = tpot.predict(X_test)\n",
    "# print(tpot.score(X_test, y_test))\n",
    "# print(tpot.fitted_pipeline_)\n",
    "\n",
    "\n",
    "# GeneticMetrics['accuracy'] = accuracy_score(y_test, pred)\n",
    "# GeneticMetrics['f1'] = f1_score(y_test, pred)\n",
    "# GeneticMetrics['roc_auc'] = roc_auc_score(y_test, pred)\n",
    "# GeneticMetrics['recall'] = recall_score(y_test, pred)\n",
    "# GeneticMetrics['precision'] = precision_score(y_test, pred)\n",
    "\n",
    "\n",
    "# for metric, score in GeneticMetrics.items():\n",
    "#     print(f'{metric}: {score}')\n",
    "    \n",
    "# print(confusion_matrix(y_test,pred))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for optmimiser, metrics in Optimisers.items():\n",
    "    print(f'\\n****{optmimiser}****')\n",
    "    for metric, score in metrics.items():\n",
    "        print(f'{metric}: {score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bio-Inspired Algorithms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opytimizer import Opytimizer\n",
    "from opytimizer.core.function import Function\n",
    "from opytimizer.optimizers.pso import PSO\n",
    "from opytimizer.spaces.search import SearchSpace\n",
    "from sklearn.model_selection import KFold, cross_val_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-25 13:27:48,014 - opytimizer.core.function — INFO — Creating class: Function.\n",
      "2020-04-25 13:27:48,015 - opytimizer.core.function — INFO — Class created.\n",
      "2020-04-25 13:27:48,016 - opytimizer.core.function — DEBUG — Fitness Function: _svm | Constraints: [] | Built: True\n",
      "2020-04-25 13:27:48,016 - opytimizer.spaces.search — INFO — Overriding class: Space -> SearchSpace.\n",
      "2020-04-25 13:27:48,017 - opytimizer.core.space — DEBUG — Running private method: build().\n",
      "2020-04-25 13:27:48,018 - opytimizer.core.space — DEBUG — Running private method: create_agents().\n",
      "2020-04-25 13:27:48,019 - opytimizer.core.space — DEBUG — Agents: 14 | Size: (8, 1) | Iterations: 200 | Lower Bound: [0.0e+00 0.0e+00 1.0e-04 0.0e+00 0.0e+00 1.5e-01 1.0e-02 0.0e+00] | Upper Bound: [4.e+00 2.e+00 1.e-02 1.e+00 1.e+00 1.e+00 1.e+00 1.e+02] | Built: True.\n",
      "2020-04-25 13:27:48,020 - opytimizer.spaces.search — DEBUG — Running private method: initialize_agents().\n",
      "2020-04-25 13:27:48,022 - opytimizer.spaces.search — DEBUG — Agents initialized.\n",
      "2020-04-25 13:27:48,023 - opytimizer.spaces.search — INFO — Class overrided.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'C': hp.uniform('C', 0.001, 10),\n",
    "        'kernel': hp.choice('kernel', ['linear', 'poly', 'rbf','sigmoid']),\n",
    "        'degree': hp.quniform('degree', 1,9,1),\n",
    "        'gamma': hp.choice ('gamma', ['scale', 'auto']),\n",
    "        'coef0' : hp.uniform ('coef0', 0.0, 10.0),\n",
    "        'shrinking' : hp.choice('shrinking', [True, False]),\n",
    "        'tol' : hp.uniform('tol', 0.001, 10.0)\n",
    "# Gathering hyperparams\n",
    "def _svm(opytimizer):\n",
    "    # Gathering hyperparams\n",
    "    kernel_ = {0: 'linear', 1: 'poly', 2: 'rbf', 3: 'sigmoid'}\n",
    "    gamma_ = {0: 'scale', 1: 'auto'}\n",
    "    shrinking_ = {0: True, 1: False}\n",
    "\n",
    "    \n",
    "    C = int(opytimizer[0][0])\n",
    "    kernel = penalty_[int(opytimizer[1][0])]\n",
    "    degree = opytimizer[2][0]\n",
    "    gamma = learning_rate_[int(opytimizer[3][0])]\n",
    "    coef0 = fit_intercept_[int(opytimizer[4][0])]\n",
    "    shrinking =  opytimizer[5][0]\n",
    "    tol = opytimizer[6][0]\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Instanciating an SVC class\n",
    "    model = SVC(C = loss, \n",
    "                          kernel = kernel,\n",
    "                          degree = degree,\n",
    "                          gamma = gamma,\n",
    "                          coef0 = coef0,\n",
    "                          shrinking = shrinking,\n",
    "                          tol = tol\n",
    "                         )\n",
    "\n",
    "\n",
    "    # Fitting model using cross-validation\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=10, n_jobs=-1)\n",
    "    \n",
    "    # Calculating scores mean\n",
    "    mean_score = np.mean(scores)\n",
    "\n",
    "    return 1 - mean_score\n",
    "\n",
    "f = Function(pointer=_svm)\n",
    "\n",
    "# Number of agents\n",
    "n_agents = 14\n",
    "\n",
    "# Number of decision variables\n",
    "n_variables = 8\n",
    "\n",
    "# Number of running iterations\n",
    "n_iterations = 200\n",
    "\n",
    "                       \n",
    "    \n",
    "# Lower and upper bounds (has to be the same size as n_variables)\n",
    "lower_bound = [0, 0, 0.0001, 0, 0, 0.15, 0.01, 0.0]\n",
    "upper_bound = [4, 2, 0.01, 1, 1, 1.0, 1.0, 100.0]\n",
    "\n",
    "s = SearchSpace(n_agents=n_agents, n_iterations=n_iterations,\n",
    "                n_variables=n_variables, lower_bound=lower_bound,\n",
    "                upper_bound=upper_bound)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-25 13:27:48,941 - opytimizer.optimizers.pso — INFO — Overriding class: Optimizer -> PSO.\n",
      "2020-04-25 13:27:48,945 - opytimizer.optimizers.pso — DEBUG — Running private method: build().\n",
      "2020-04-25 13:27:48,946 - opytimizer.optimizers.pso — DEBUG — Algorithm: PSO | Hyperparameters: w = 0.7, c1 = 1.7, c2 = 1.7 | Built: True.\n",
      "2020-04-25 13:27:48,946 - opytimizer.optimizers.pso — INFO — Class overrided.\n",
      "2020-04-25 13:27:48,947 - opytimizer.opytimizer — INFO — Creating class: Opytimizer.\n",
      "2020-04-25 13:27:48,947 - opytimizer.opytimizer — DEBUG — Space: <opytimizer.spaces.search.SearchSpace object at 0x7f20426b04a8> | Optimizer: <opytimizer.optimizers.pso.PSO object at 0x7f20414fa780> | Function: <opytimizer.core.function.Function object at 0x7f2041578c18>.\n",
      "2020-04-25 13:27:48,948 - opytimizer.opytimizer — INFO — Class created.\n",
      "2020-04-25 13:27:48,948 - opytimizer.opytimizer — INFO — Starting optimization task.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-30c6c0c62a94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Running the optimization task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPSO_opitmise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/opytimizer/opytimizer.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, store_best_only, pre_evaluation_hook)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;31m# Starting optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mopt_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstore_best_only\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_evaluation_hook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;31m# Ending timer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/opytimizer/optimizers/pso.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, space, function, store_best_only, pre_evaluation_hook)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# Initial search space evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_position\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0;31m# We will define a History object for further dumping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/opytimizer/optimizers/pso.py\u001b[0m in \u001b[0;36m_evaluate\u001b[0;34m(self, space, function, local_position)\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m             \u001b[0;31m# Calculate the fitness value of current agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m             \u001b[0mfit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpointer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;31m# If fitness is better than agent's best fit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/opytimizer/core/function.py\u001b[0m in \u001b[0;36mf\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;31m# If all constraints are satisfied, return the fitness function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mpointer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;31m# Returns the function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-2260f1eb40c2>\u001b[0m in \u001b[0;36m_svm\u001b[0;34m(opytimizer)\u001b[0m\n\u001b[1;32m     25\u001b[0m                           \u001b[0ml1_ratio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml1_ratio\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                           \u001b[0meta0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meta0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                           \u001b[0mpower_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpower_t\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m                          )\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'loss'"
     ]
    }
   ],
   "source": [
    "# Hyperparameters for the optimizer\n",
    "hyperparams = {\n",
    "    'w': 0.7,\n",
    "    'c1': 1.7,\n",
    "    'c2': 1.7\n",
    "}\n",
    "\n",
    "# Finally, we can create an Opytimizer class\n",
    "p = PSO(hyperparams=hyperparams)\n",
    "PSO_opitmise = Opytimizer(space=s, optimizer=p, function=f)\n",
    "\n",
    "# Running the optimization task\n",
    "history = PSO_opitmise.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-d43fbe3b252b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_agent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mbioOptimisers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mPSOMetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbioOptimisers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Partical Swarm'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPSOMetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "best = np.array(history.best_agent[-1][0]).ravel()\n",
    "\n",
    "bioOptimisers = {}\n",
    "PSOMetrics = {}\n",
    "bioOptimisers['Partical Swarm'] = PSOMetrics\n",
    "\n",
    "\n",
    "loss_ = {0: 'hinge', 1: 'log', 2: 'modified_huber', 3: 'squared_hinge', 4: 'perceptron'}\n",
    "penalty_ = {0: 'l2', 1: 'l1', 2: 'elasticnet'}\n",
    "learning_rate_ = {0: 'invscaling', 1: 'constant'}\n",
    "fit_intercept_ = {0: True, 1: False}\n",
    "\n",
    "\n",
    "svm = SVC(loss = loss_[int(best[0])], \n",
    "                  penalty = penalty_[int(best[1])], \n",
    "                  alpha = best[2], \n",
    "                  learning_rate = learning_rate_[int(best[3])], \n",
    "                  fit_intercept = fit_intercept_[int(best[4])], \n",
    "                  l1_ratio = best[5],\n",
    "                  eta0 = best[6],\n",
    "                  power_t = best[7]).fit(X_train, y_train)\n",
    "\n",
    "                              \n",
    "pred = svm.predict(X_test)\n",
    "\n",
    "\n",
    "PSOMetrics['accuracy'] = accuracy_score(y_test, pred)\n",
    "PSOMetrics['f1'] = f1_score(y_test, pred)\n",
    "PSOMetrics['roc_auc'] = roc_auc_score(y_test, pred)\n",
    "PSOMetrics['recall'] = recall_score(y_test, pred)\n",
    "PSOMetrics['precision'] = precision_score(y_test, pred)\n",
    "\n",
    "for metric, score in PSOMetrics.items():\n",
    "    print(f'{metric}: {score}')\n",
    "    \n",
    "print(confusion_matrix(y_test,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-25 13:27:51,807 - opytimizer.optimizers.abc — INFO — Overriding class: Optimizer -> ABC.\n",
      "2020-04-25 13:27:51,807 - opytimizer.optimizers.abc — DEBUG — Running private method: build().\n",
      "2020-04-25 13:27:51,807 - opytimizer.optimizers.abc — DEBUG — Algorithm: ABC | Hyperparameters: n_trials = 10.\n",
      "2020-04-25 13:27:51,809 - opytimizer.optimizers.abc — INFO — Class overrided.\n",
      "2020-04-25 13:27:51,810 - opytimizer.opytimizer — INFO — Creating class: Opytimizer.\n",
      "2020-04-25 13:27:51,810 - opytimizer.opytimizer — DEBUG — Space: <opytimizer.spaces.search.SearchSpace object at 0x7f20426b04a8> | Optimizer: <opytimizer.optimizers.abc.ABC object at 0x7f2041578438> | Function: <opytimizer.core.function.Function object at 0x7f2041578c18>.\n",
      "2020-04-25 13:27:51,811 - opytimizer.opytimizer — INFO — Class created.\n",
      "2020-04-25 13:27:51,811 - opytimizer.opytimizer — INFO — Starting optimization task.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-a0f1f17fc5b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mABC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyperparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhyperparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mABC_optimise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOpytimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mABC_optimise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/opytimizer/opytimizer.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, store_best_only, pre_evaluation_hook)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;31m# Starting optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mopt_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstore_best_only\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_evaluation_hook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;31m# Ending timer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/opytimizer/optimizers/abc.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, space, function, store_best_only, pre_evaluation_hook)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0;31m# Initial search space evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0;31m# We will define a History object for further dumping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/opytimizer/core/optimizer.py\u001b[0m in \u001b[0;36m_evaluate\u001b[0;34m(self, space, function)\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0magent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;31m# Calculate the fitness value of current agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpointer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0;31m# If agent's fitness is better than global fitness\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/opytimizer/core/function.py\u001b[0m in \u001b[0;36mf\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;31m# If all constraints are satisfied, return the fitness function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mpointer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;31m# Returns the function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-2260f1eb40c2>\u001b[0m in \u001b[0;36m_svm\u001b[0;34m(opytimizer)\u001b[0m\n\u001b[1;32m     25\u001b[0m                           \u001b[0ml1_ratio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml1_ratio\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                           \u001b[0meta0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meta0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                           \u001b[0mpower_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpower_t\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m                          )\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'loss'"
     ]
    }
   ],
   "source": [
    "from opytimizer.optimizers.abc import ABC\n",
    "\n",
    "hyperparams = {\n",
    "    'n_trials': 10\n",
    "}\n",
    "\n",
    "# Creating an ABC optimizer\n",
    "p =  ABC(hyperparams=hyperparams)\n",
    "ABC_optimise = Opytimizer(space=s, optimizer=p, function=f)\n",
    "history = ABC_optimise.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = np.array(history.best_agent[-1][0]).ravel()\n",
    "\n",
    "ABCMetrics = {}\n",
    "bioOptimisers['ABC'] = ABCMetrics\n",
    "\n",
    "\n",
    "\n",
    "svm = SVC(loss = loss_[int(best[0])], \n",
    "                  penalty = penalty_[int(best[1])], \n",
    "                  alpha = best[2], \n",
    "                  learning_rate = learning_rate_[int(best[3])], \n",
    "                  fit_intercept = fit_intercept_[int(best[4])], \n",
    "                  l1_ratio = best[5],\n",
    "                  eta0 = best[6],\n",
    "                  power_t = best[7]).fit(X_train, y_train)\n",
    "\n",
    "                              \n",
    "pred = svm.predict(X_test)\n",
    "\n",
    "ABCMetrics['accuracy'] = accuracy_score(y_test, pred)\n",
    "ABCMetrics['f1'] = f1_score(y_test, pred)\n",
    "ABCMetrics['roc_auc'] = roc_auc_score(y_test, pred)\n",
    "ABCMetrics['recall'] = recall_score(y_test, pred)\n",
    "ABCMetrics['precision'] = precision_score(y_test, pred)\n",
    "\n",
    "for metric, score in ABCMetrics.items():\n",
    "    print(f'{metric}: {score}')\n",
    "    \n",
    "print(confusion_matrix(y_test,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from opytimizer.optimizers.fa import FA\n",
    "\n",
    "hyperparams = {\n",
    "    'alpha': 0.5,\n",
    "    'beta': 0.2,\n",
    "    'gamma': 1.0\n",
    "}\n",
    "\n",
    "# Creating a FA optimizer\n",
    "p = FA(hyperparams=hyperparams)\n",
    "\n",
    "FA_optimise = Opytimizer(space=s, optimizer=p, function=f)\n",
    "history = ABC_optimise.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = np.array(history.best_agent[-1][0]).ravel()\n",
    "\n",
    "FAMetrics = {}\n",
    "bioOptimisers['FA'] = FAMetrics\n",
    "\n",
    "\n",
    "crit = {0: 'entropy', 1: 'gini'}\n",
    "feat = {0: 'auto', 1: 'sqrt', 2: 'log2', 3: None}\n",
    "\n",
    "\n",
    "sgd = SVC(loss = loss_[int(best[0])], \n",
    "                  penalty = penalty_[int(best[1])], \n",
    "                  alpha = best[2], \n",
    "                  learning_rate = learning_rate_[int(best[3])], \n",
    "                  fit_intercept = fit_intercept_[int(best[4])], \n",
    "                  l1_ratio = best[5],\n",
    "                  eta0 = best[6],\n",
    "                  power_t = best[7]).fit(X_train, y_train)\n",
    "\n",
    "                              \n",
    "pred = sgd.predict(X_test)\n",
    "\n",
    "FAMetrics['accuracy'] = accuracy_score(y_test, pred)\n",
    "FAMetrics['f1'] = f1_score(y_test, pred)\n",
    "FAMetrics['roc_auc'] = roc_auc_score(y_test, pred)\n",
    "FAMetrics['recall'] = recall_score(y_test, pred)\n",
    "FAMetrics['precision'] = precision_score(y_test, pred)\n",
    "\n",
    "for metric, score in FAMetrics.items():\n",
    "    print(f'{metric}: {score}')\n",
    "    \n",
    "print(confusion_matrix(y_test,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
